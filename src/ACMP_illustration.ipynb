{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/miniconda3/envs/ACMP/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.5.\n"
     ]
    }
   ],
   "source": [
    "from GNN import GNN\n",
    "from run_GNN import get_optimizer, test,  train, main\n",
    "from best_params import best_params_dict\n",
    "from data import get_dataset\n",
    "from utils import ROOT_DIR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "opt = np.load('config.npy',allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "def plot_state(init_state,final_state):\n",
    "    fig, ax = plt.subplots(2)\n",
    "    ax[0].scatter(init_state[:,0].cpu().numpy(),init_state[:,1].cpu().numpy())\n",
    "    ax[1].scatter(final_state[:,0].cpu().numpy(),final_state[:,1].cpu().numpy())\n",
    "    ax[1].set_xlim(-3,3)\n",
    "    ax[1].set_ylim(-3,3)\n",
    "    ax[0].set_xlim(-3,3)\n",
    "    ax[0].set_ylim(-3,3)\n",
    "    # plt.xlim(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN +Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GCN\n",
      "using GCN\n",
      "using Cora dataset\n",
      "Epoch: 001, Runtime 1.243749, Loss 1.992528, forward nfe 44, backward nfe 0, Train: 0.6429, Val: 0.4426, Test: 0.4404, Best time: 1.5358\n",
      "Epoch: 002, Runtime 0.412426, Loss 1.274630, forward nfe 552, backward nfe 0, Train: 0.8000, Val: 0.6890, Test: 0.7061, Best time: 3.7680\n",
      "Epoch: 003, Runtime 0.395289, Loss 0.679188, forward nfe 1156, backward nfe 0, Train: 0.8000, Val: 0.6890, Test: 0.7061, Best time: 3.0000\n",
      "Epoch: 004, Runtime 0.343189, Loss 0.564593, forward nfe 1766, backward nfe 0, Train: 0.9143, Val: 0.7603, Test: 0.7583, Best time: 4.9796\n",
      "Epoch: 005, Runtime 0.378074, Loss 0.306178, forward nfe 2298, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 4.9055\n",
      "Epoch: 006, Runtime 0.363142, Loss 0.102398, forward nfe 2884, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 007, Runtime 0.380083, Loss 0.135409, forward nfe 3410, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 008, Runtime 0.392586, Loss 0.119643, forward nfe 4032, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 009, Runtime 0.379131, Loss 0.126955, forward nfe 4624, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 010, Runtime 0.359250, Loss 0.079136, forward nfe 5210, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 011, Runtime 0.368024, Loss 0.030876, forward nfe 5754, backward nfe 0, Train: 0.9714, Val: 0.7941, Test: 0.8038, Best time: 3.0000\n",
      "Epoch: 012, Runtime 0.393223, Loss 0.026095, forward nfe 6304, backward nfe 0, Train: 0.9786, Val: 0.7949, Test: 0.8046, Best time: 5.7831\n",
      "Epoch: 013, Runtime 0.420330, Loss 0.046247, forward nfe 6890, backward nfe 0, Train: 0.9786, Val: 0.7949, Test: 0.8046, Best time: 3.0000\n",
      "Epoch: 014, Runtime 0.374401, Loss 0.046581, forward nfe 7554, backward nfe 0, Train: 0.9571, Val: 0.7978, Test: 0.8055, Best time: 3.0000\n",
      "Epoch: 015, Runtime 0.385126, Loss 0.031222, forward nfe 8110, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 7.2368\n",
      "Epoch: 016, Runtime 0.351352, Loss 0.009198, forward nfe 8702, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 017, Runtime 0.428450, Loss 0.010812, forward nfe 9246, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 018, Runtime 0.394994, Loss 0.015842, forward nfe 9880, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 019, Runtime 0.388189, Loss 0.026708, forward nfe 10472, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 020, Runtime 0.379398, Loss 0.012970, forward nfe 11088, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 021, Runtime 0.390930, Loss 0.010422, forward nfe 11644, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 022, Runtime 0.420344, Loss 0.012853, forward nfe 12242, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 023, Runtime 0.338232, Loss 0.009322, forward nfe 12876, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 024, Runtime 0.443366, Loss 0.050137, forward nfe 13378, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 025, Runtime 0.382557, Loss 0.022879, forward nfe 14030, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 026, Runtime 0.339820, Loss 0.028580, forward nfe 14622, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 027, Runtime 0.378736, Loss 0.016863, forward nfe 15136, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 028, Runtime 0.493430, Loss 0.009414, forward nfe 15764, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 029, Runtime 0.378415, Loss 0.009366, forward nfe 16356, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 030, Runtime 0.390775, Loss 0.006093, forward nfe 16894, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 031, Runtime 0.827791, Loss 0.027942, forward nfe 17480, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 032, Runtime 0.765709, Loss 0.005097, forward nfe 18090, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 033, Runtime 0.707220, Loss 0.023498, forward nfe 18688, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 034, Runtime 0.796578, Loss 0.008677, forward nfe 19220, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 035, Runtime 0.685236, Loss 0.018134, forward nfe 19836, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 036, Runtime 0.797121, Loss 0.010341, forward nfe 20362, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 037, Runtime 0.766833, Loss 0.018820, forward nfe 20984, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 038, Runtime 0.711503, Loss 0.007878, forward nfe 21552, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 039, Runtime 0.732483, Loss 0.028755, forward nfe 22126, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 040, Runtime 0.648960, Loss 0.014349, forward nfe 22676, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 041, Runtime 0.662712, Loss 0.011169, forward nfe 23178, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 042, Runtime 0.808043, Loss 0.004253, forward nfe 23692, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 043, Runtime 0.707554, Loss 0.004534, forward nfe 24320, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 044, Runtime 0.784167, Loss 0.021502, forward nfe 24876, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 045, Runtime 0.696961, Loss 0.012991, forward nfe 25486, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 046, Runtime 0.755153, Loss 0.010050, forward nfe 26018, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 047, Runtime 0.641610, Loss 0.009958, forward nfe 26574, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 048, Runtime 0.733180, Loss 0.022013, forward nfe 27100, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 049, Runtime 0.795975, Loss 0.012504, forward nfe 27668, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 050, Runtime 0.811096, Loss 0.018392, forward nfe 28290, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 051, Runtime 0.772702, Loss 0.008703, forward nfe 28936, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 052, Runtime 0.733519, Loss 0.016030, forward nfe 29516, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 053, Runtime 0.670755, Loss 0.008780, forward nfe 30066, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 054, Runtime 0.686992, Loss 0.027314, forward nfe 30610, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 055, Runtime 0.687201, Loss 0.008074, forward nfe 31100, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 056, Runtime 0.781679, Loss 0.013212, forward nfe 31638, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 057, Runtime 0.797636, Loss 0.014331, forward nfe 32254, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 058, Runtime 0.687956, Loss 0.012594, forward nfe 32870, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 059, Runtime 0.746459, Loss 0.021583, forward nfe 33390, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 060, Runtime 0.742725, Loss 0.011668, forward nfe 33934, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 061, Runtime 0.786378, Loss 0.007495, forward nfe 34532, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 062, Runtime 0.782104, Loss 0.011693, forward nfe 35166, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 063, Runtime 0.691958, Loss 0.008749, forward nfe 35758, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 064, Runtime 0.691197, Loss 0.012355, forward nfe 36278, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 065, Runtime 0.739373, Loss 0.012750, forward nfe 36804, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 066, Runtime 0.742713, Loss 0.030030, forward nfe 37372, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 067, Runtime 0.826758, Loss 0.021780, forward nfe 37946, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 068, Runtime 0.723904, Loss 0.005446, forward nfe 38580, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 069, Runtime 0.707058, Loss 0.007409, forward nfe 39136, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 070, Runtime 0.713107, Loss 0.010123, forward nfe 39686, backward nfe 0, Train: 0.9714, Val: 0.7985, Test: 0.8088, Best time: 3.0000\n",
      "Epoch: 071, Runtime 0.864695, Loss 0.017883, forward nfe 40260, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 8.8477\n",
      "Epoch: 072, Runtime 0.735293, Loss 0.010675, forward nfe 40912, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 073, Runtime 0.736919, Loss 0.007991, forward nfe 41462, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 074, Runtime 0.702033, Loss 0.011577, forward nfe 42042, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 075, Runtime 0.858249, Loss 0.006569, forward nfe 42610, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 076, Runtime 0.699109, Loss 0.012985, forward nfe 43238, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 077, Runtime 0.360058, Loss 0.010246, forward nfe 43794, backward nfe 0, Train: 0.9571, Val: 0.8007, Test: 0.8071, Best time: 3.0000\n",
      "Epoch: 078, Runtime 0.355329, Loss 0.016697, forward nfe 44350, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 8.4829\n",
      "Epoch: 079, Runtime 0.368954, Loss 0.018648, forward nfe 44888, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 080, Runtime 0.346790, Loss 0.009758, forward nfe 45372, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 081, Runtime 0.370389, Loss 0.008742, forward nfe 45898, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 082, Runtime 0.369896, Loss 0.008199, forward nfe 46490, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 083, Runtime 0.317997, Loss 0.014999, forward nfe 47028, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 084, Runtime 0.373679, Loss 0.017314, forward nfe 47506, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 085, Runtime 0.331712, Loss 0.023133, forward nfe 48068, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 086, Runtime 0.373628, Loss 0.012394, forward nfe 48570, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 087, Runtime 0.345185, Loss 0.006864, forward nfe 49132, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 088, Runtime 0.396224, Loss 0.008909, forward nfe 49640, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 089, Runtime 0.335449, Loss 0.016391, forward nfe 50226, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 090, Runtime 0.304457, Loss 0.006051, forward nfe 50722, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 091, Runtime 0.367228, Loss 0.028501, forward nfe 51176, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 092, Runtime 0.340139, Loss 0.009436, forward nfe 51744, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 093, Runtime 0.361268, Loss 0.009726, forward nfe 52228, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 094, Runtime 0.307789, Loss 0.010970, forward nfe 52802, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 095, Runtime 0.413591, Loss 0.010539, forward nfe 53268, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 096, Runtime 0.332791, Loss 0.010313, forward nfe 53884, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 097, Runtime 0.383114, Loss 0.019719, forward nfe 54410, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 098, Runtime 0.399839, Loss 0.007319, forward nfe 54978, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "Epoch: 099, Runtime 0.320119, Loss 0.010681, forward nfe 55570, backward nfe 0, Train: 0.9714, Val: 0.8029, Test: 0.8079, Best time: 3.0000\n",
      "best val accuracy 0.802941 with test accuracy 0.807947 at epoch 78 and best time 3.000000\n"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'GCN'\n",
    "opt['time'] = 3\n",
    "_,_,_,init_state,final_state = main(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversmoothing problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "using GCN\n",
      "using GCN\n",
      "using Cora dataset\n",
      "Epoch: 001, Runtime 6.077875, Loss 167466205184.000000, forward nfe 3290, backward nfe 0, Train: 0.1714, Val: 0.1647, Test: 0.1664, Best time: 0.5574\n",
      "Epoch: 002, Runtime 4.510000, Loss 3535523807232.000000, forward nfe 7110, backward nfe 0, Train: 0.2571, Val: 0.3463, Test: 0.3419, Best time: 8.7825\n",
      "Epoch: 003, Runtime 4.400140, Loss 4621555728384.000000, forward nfe 10834, backward nfe 0, Train: 0.2571, Val: 0.3463, Test: 0.3419, Best time: 30.0000\n",
      "Epoch: 004, Runtime 4.203169, Loss 3429175918592.000000, forward nfe 14294, backward nfe 0, Train: 0.2571, Val: 0.3463, Test: 0.3419, Best time: 30.0000\n",
      "Epoch: 005, Runtime 4.025541, Loss 3354903183360.000000, forward nfe 17658, backward nfe 0, Train: 0.2571, Val: 0.3463, Test: 0.3419, Best time: 30.0000\n",
      "Epoch: 006, Runtime 4.323045, Loss 3205122228224.000000, forward nfe 21820, backward nfe 0, Train: 0.2571, Val: 0.3463, Test: 0.3419, Best time: 30.0000\n",
      "Epoch: 007, Runtime 2.363302, Loss 2666715414528.000000, forward nfe 24974, backward nfe 0, Train: 0.3286, Val: 0.3794, Test: 0.3882, Best time: 7.2668\n",
      "Epoch: 008, Runtime 2.229813, Loss 2232228773888.000000, forward nfe 27984, backward nfe 0, Train: 0.4429, Val: 0.4507, Test: 0.4512, Best time: 1.6367\n",
      "Epoch: 009, Runtime 3.308442, Loss 1717104672768.000000, forward nfe 32416, backward nfe 0, Train: 0.4857, Val: 0.5816, Test: 0.5588, Best time: 3.2671\n",
      "Epoch: 010, Runtime 2.603074, Loss 1148641607680.000000, forward nfe 35948, backward nfe 0, Train: 0.4857, Val: 0.5816, Test: 0.5588, Best time: 30.0000\n",
      "Epoch: 011, Runtime 2.843121, Loss 1289920315392.000000, forward nfe 38934, backward nfe 0, Train: 0.4857, Val: 0.5816, Test: 0.5588, Best time: 30.0000\n",
      "Epoch: 012, Runtime 4.774550, Loss 1009250467840.000000, forward nfe 42892, backward nfe 0, Train: 0.4857, Val: 0.5816, Test: 0.5588, Best time: 30.0000\n",
      "Epoch: 013, Runtime 4.156486, Loss 679458111488.000000, forward nfe 46382, backward nfe 0, Train: 0.4857, Val: 0.5816, Test: 0.5588, Best time: 30.0000\n",
      "Epoch: 014, Runtime 3.894756, Loss 392600322048.000000, forward nfe 49662, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 5.3422\n",
      "Epoch: 015, Runtime 5.952563, Loss 113967104000.000000, forward nfe 54502, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 016, Runtime 4.749547, Loss 83443318784.000000, forward nfe 58496, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 017, Runtime 4.921550, Loss 216253513728.000000, forward nfe 62616, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 018, Runtime 4.491792, Loss 217356271616.000000, forward nfe 66214, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 019, Runtime 3.850394, Loss 190827069440.000000, forward nfe 69782, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 020, Runtime 3.279721, Loss 159520751616.000000, forward nfe 74244, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 021, Runtime 2.939782, Loss 129838391296.000000, forward nfe 78190, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 022, Runtime 3.334944, Loss 144508026880.000000, forward nfe 82592, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 023, Runtime 2.845543, Loss 102692691968.000000, forward nfe 86256, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 024, Runtime 4.517813, Loss 91824881664.000000, forward nfe 90112, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 025, Runtime 4.713440, Loss 83809419264.000000, forward nfe 94094, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 026, Runtime 5.487211, Loss 55193493504.000000, forward nfe 98760, backward nfe 0, Train: 0.8143, Val: 0.7000, Test: 0.6656, Best time: 30.0000\n",
      "Epoch: 027, Runtime 5.810238, Loss 43115302912.000000, forward nfe 103606, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 8.5562\n",
      "Epoch: 028, Runtime 4.888610, Loss 30580553728.000000, forward nfe 107756, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 029, Runtime 3.709077, Loss 46149296128.000000, forward nfe 110898, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 030, Runtime 3.875770, Loss 68989116416.000000, forward nfe 114226, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 031, Runtime 4.523629, Loss 56958177280.000000, forward nfe 118004, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 032, Runtime 3.397084, Loss 41734590464.000000, forward nfe 122286, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 033, Runtime 2.555495, Loss 44038696960.000000, forward nfe 125872, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 034, Runtime 2.494828, Loss 46785560576.000000, forward nfe 129164, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 035, Runtime 2.453136, Loss 43412475904.000000, forward nfe 132612, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 036, Runtime 3.597211, Loss 36450820096.000000, forward nfe 136948, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 037, Runtime 3.693116, Loss 30942121984.000000, forward nfe 140066, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 038, Runtime 3.501346, Loss 38666985472.000000, forward nfe 142998, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 039, Runtime 4.042057, Loss 34770321408.000000, forward nfe 146446, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 040, Runtime 4.210982, Loss 30118637568.000000, forward nfe 149966, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 041, Runtime 3.852172, Loss 25116057600.000000, forward nfe 153216, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 042, Runtime 4.106976, Loss 29656406016.000000, forward nfe 156466, backward nfe 0, Train: 0.8786, Val: 0.7250, Test: 0.6904, Best time: 30.0000\n",
      "Epoch: 043, Runtime 5.129932, Loss 31236501504.000000, forward nfe 160796, backward nfe 0, Train: 0.8571, Val: 0.7346, Test: 0.6978, Best time: 30.0000\n",
      "Epoch: 044, Runtime 3.662564, Loss 26982162432.000000, forward nfe 163890, backward nfe 0, Train: 0.8571, Val: 0.7404, Test: 0.7070, Best time: 30.0000\n",
      "Epoch: 045, Runtime 4.563614, Loss 26142971904.000000, forward nfe 167758, backward nfe 0, Train: 0.8500, Val: 0.7463, Test: 0.7202, Best time: 9.4036\n",
      "Epoch: 046, Runtime 2.946112, Loss 30291660800.000000, forward nfe 171608, backward nfe 0, Train: 0.8643, Val: 0.7529, Test: 0.7293, Best time: 9.3876\n",
      "Epoch: 047, Runtime 2.166984, Loss 27079243776.000000, forward nfe 174522, backward nfe 0, Train: 0.8643, Val: 0.7529, Test: 0.7293, Best time: 30.0000\n",
      "Epoch: 048, Runtime 2.735954, Loss 20862994432.000000, forward nfe 178216, backward nfe 0, Train: 0.8786, Val: 0.7640, Test: 0.7359, Best time: 30.0000\n",
      "Epoch: 049, Runtime 3.071120, Loss 22705240064.000000, forward nfe 182228, backward nfe 0, Train: 0.8714, Val: 0.7654, Test: 0.7409, Best time: 8.9843\n",
      "Epoch: 050, Runtime 2.875525, Loss 21886177280.000000, forward nfe 185646, backward nfe 0, Train: 0.8786, Val: 0.7669, Test: 0.7409, Best time: 8.6976\n",
      "Epoch: 051, Runtime 4.008911, Loss 23560916992.000000, forward nfe 188986, backward nfe 0, Train: 0.8786, Val: 0.7669, Test: 0.7409, Best time: 30.0000\n",
      "Epoch: 052, Runtime 3.896176, Loss 21631330304.000000, forward nfe 192272, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 053, Runtime 4.686713, Loss 20201625600.000000, forward nfe 196164, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 054, Runtime 3.808063, Loss 21185515520.000000, forward nfe 199342, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 055, Runtime 5.116399, Loss 18858498048.000000, forward nfe 203684, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 056, Runtime 4.417423, Loss 16303192064.000000, forward nfe 207366, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 057, Runtime 5.252792, Loss 14727638016.000000, forward nfe 211798, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 058, Runtime 5.151943, Loss 14309278720.000000, forward nfe 216134, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 059, Runtime 2.931214, Loss 15556721664.000000, forward nfe 219570, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 060, Runtime 2.663848, Loss 16776011776.000000, forward nfe 223102, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 061, Runtime 2.579725, Loss 10902735872.000000, forward nfe 226616, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 062, Runtime 2.939129, Loss 10937952256.000000, forward nfe 230562, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 063, Runtime 2.652767, Loss 10565353472.000000, forward nfe 233884, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 064, Runtime 3.902481, Loss 10612682752.000000, forward nfe 237194, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 065, Runtime 4.655871, Loss 9540668416.000000, forward nfe 241152, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 066, Runtime 3.875250, Loss 7979030528.000000, forward nfe 244396, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 067, Runtime 4.769943, Loss 7010219520.000000, forward nfe 248366, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 068, Runtime 3.615829, Loss 5450973696.000000, forward nfe 251406, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 069, Runtime 4.954839, Loss 9777973248.000000, forward nfe 255634, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 070, Runtime 3.666017, Loss 9841954816.000000, forward nfe 258686, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 071, Runtime 5.227827, Loss 7250207232.000000, forward nfe 263088, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 072, Runtime 4.128557, Loss 5723986432.000000, forward nfe 266974, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 073, Runtime 2.736970, Loss 5021061632.000000, forward nfe 270674, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 074, Runtime 2.518945, Loss 4271931904.000000, forward nfe 273936, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 075, Runtime 2.183946, Loss 5168248832.000000, forward nfe 276850, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 076, Runtime 2.291152, Loss 3900282368.000000, forward nfe 279932, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 077, Runtime 3.232511, Loss 3908705536.000000, forward nfe 283416, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 078, Runtime 4.588803, Loss 3287280640.000000, forward nfe 287152, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 079, Runtime 4.384624, Loss 3889465600.000000, forward nfe 290840, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 080, Runtime 4.279391, Loss 2778883840.000000, forward nfe 294462, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 081, Runtime 4.232565, Loss 2464489984.000000, forward nfe 297964, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 082, Runtime 4.247594, Loss 2418003968.000000, forward nfe 301538, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 083, Runtime 4.268015, Loss 2106651264.000000, forward nfe 305118, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 084, Runtime 3.843843, Loss 2169605888.000000, forward nfe 308362, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 085, Runtime 3.924681, Loss 2341594368.000000, forward nfe 311594, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 086, Runtime 4.136544, Loss 2530251520.000000, forward nfe 315708, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 087, Runtime 2.387054, Loss 2104394880.000000, forward nfe 318922, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 088, Runtime 3.124815, Loss 1795075968.000000, forward nfe 323054, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 089, Runtime 2.407587, Loss 1730987136.000000, forward nfe 326316, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 090, Runtime 2.580782, Loss 1437523968.000000, forward nfe 329782, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 091, Runtime 3.382861, Loss 1192318720.000000, forward nfe 333260, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 092, Runtime 3.735371, Loss 996867968.000000, forward nfe 336330, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 093, Runtime 5.031049, Loss 1179085824.000000, forward nfe 340492, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 094, Runtime 3.804792, Loss 1402281216.000000, forward nfe 343706, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 095, Runtime 4.120455, Loss 888199488.000000, forward nfe 347172, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 096, Runtime 4.330624, Loss 1163728000.000000, forward nfe 350752, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 097, Runtime 4.758066, Loss 473531232.000000, forward nfe 354746, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 098, Runtime 4.627062, Loss 786320064.000000, forward nfe 358710, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "Epoch: 099, Runtime 3.996275, Loss 894453824.000000, forward nfe 362080, backward nfe 0, Train: 0.8714, Val: 0.7684, Test: 0.7467, Best time: 30.0000\n",
      "best val accuracy 0.768382 with test accuracy 0.746689 at epoch 52 and best time 30.000000\n"
     ]
    }
   ],
   "source": [
    "opt['max_nfe'] = 10000\n",
    "opt['function'] = 'GCN'\n",
    "opt['time'] = 30\n",
    "_,_,_,init_state,final_state  = main(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of feature. The first coloum is the inital state and the second coloum is feature when T = 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_state(init_state,final_state)\n",
    "# print(final_state.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using Cora dataset\n",
      "Epoch: 001, Runtime 0.183749, Loss 1.945443, forward nfe 56, backward nfe 0, Train: 0.6214, Val: 0.3713, Test: 0.4081, Best time: 0.0967\n",
      "Epoch: 002, Runtime 0.191098, Loss 1.777301, forward nfe 252, backward nfe 0, Train: 0.8929, Val: 0.6875, Test: 0.6978, Best time: 5.6603\n",
      "Epoch: 003, Runtime 0.213445, Loss 1.476168, forward nfe 442, backward nfe 0, Train: 0.9071, Val: 0.8096, Test: 0.8237, Best time: 33.5121\n",
      "Epoch: 004, Runtime 0.206426, Loss 1.126723, forward nfe 650, backward nfe 0, Train: 0.9286, Val: 0.8243, Test: 0.8377, Best time: 30.9228\n",
      "Epoch: 005, Runtime 0.216104, Loss 0.816678, forward nfe 864, backward nfe 0, Train: 0.9286, Val: 0.8243, Test: 0.8377, Best time: 18.0000\n",
      "Epoch: 006, Runtime 0.212647, Loss 0.594055, forward nfe 1090, backward nfe 0, Train: 0.9286, Val: 0.8257, Test: 0.8320, Best time: 44.6247\n",
      "Epoch: 007, Runtime 0.239701, Loss 0.430222, forward nfe 1334, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 32.5749\n",
      "Epoch: 008, Runtime 0.244987, Loss 0.302605, forward nfe 1584, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 009, Runtime 0.238654, Loss 0.241824, forward nfe 1852, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 010, Runtime 0.247982, Loss 0.186647, forward nfe 2114, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 011, Runtime 0.244720, Loss 0.180315, forward nfe 2376, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 012, Runtime 0.241951, Loss 0.163614, forward nfe 2638, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 013, Runtime 0.239221, Loss 0.137002, forward nfe 2900, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 014, Runtime 0.233598, Loss 0.165273, forward nfe 3162, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 015, Runtime 0.227720, Loss 0.154540, forward nfe 3418, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 016, Runtime 0.218427, Loss 0.123917, forward nfe 3674, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 017, Runtime 0.227034, Loss 0.227640, forward nfe 3924, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 018, Runtime 0.212839, Loss 0.151463, forward nfe 4174, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 019, Runtime 0.223164, Loss 0.206657, forward nfe 4424, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 020, Runtime 0.231734, Loss 0.163878, forward nfe 4674, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 021, Runtime 0.229476, Loss 0.226447, forward nfe 4930, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 022, Runtime 0.227342, Loss 0.185712, forward nfe 5186, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 023, Runtime 0.228221, Loss 0.178389, forward nfe 5436, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 024, Runtime 0.219159, Loss 0.170941, forward nfe 5680, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 025, Runtime 0.221396, Loss 0.147814, forward nfe 5924, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 026, Runtime 0.225601, Loss 0.172796, forward nfe 6168, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 027, Runtime 0.212455, Loss 0.172098, forward nfe 6418, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 028, Runtime 0.220998, Loss 0.128263, forward nfe 6680, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 029, Runtime 0.216857, Loss 0.130182, forward nfe 6930, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 030, Runtime 0.230074, Loss 0.138031, forward nfe 7180, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 031, Runtime 0.238662, Loss 0.159435, forward nfe 7436, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 032, Runtime 0.234422, Loss 0.121680, forward nfe 7692, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 033, Runtime 0.240499, Loss 0.114894, forward nfe 7948, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 034, Runtime 0.228825, Loss 0.160786, forward nfe 8198, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 035, Runtime 0.226365, Loss 0.113849, forward nfe 8454, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 036, Runtime 0.227161, Loss 0.116799, forward nfe 8704, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 037, Runtime 0.240992, Loss 0.102802, forward nfe 8960, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 038, Runtime 0.234720, Loss 0.152476, forward nfe 9228, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 039, Runtime 0.240622, Loss 0.118176, forward nfe 9490, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 040, Runtime 0.234241, Loss 0.117208, forward nfe 9758, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 041, Runtime 0.247969, Loss 0.131599, forward nfe 10020, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 042, Runtime 0.256083, Loss 0.112092, forward nfe 10282, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 043, Runtime 0.245179, Loss 0.171119, forward nfe 10562, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 044, Runtime 0.233460, Loss 0.137213, forward nfe 10836, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 045, Runtime 0.232941, Loss 0.142771, forward nfe 11098, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 046, Runtime 0.228888, Loss 0.125907, forward nfe 11354, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 047, Runtime 0.271394, Loss 0.108957, forward nfe 11640, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 048, Runtime 0.271230, Loss 0.125418, forward nfe 11920, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 049, Runtime 0.250073, Loss 0.121358, forward nfe 12200, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 050, Runtime 0.218183, Loss 0.136632, forward nfe 12468, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 051, Runtime 0.261025, Loss 0.140005, forward nfe 12736, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 052, Runtime 0.444583, Loss 0.125845, forward nfe 12992, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 053, Runtime 0.391486, Loss 0.094320, forward nfe 13254, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 054, Runtime 0.424938, Loss 0.131907, forward nfe 13516, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 055, Runtime 0.383171, Loss 0.094354, forward nfe 13772, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 056, Runtime 0.455086, Loss 0.134896, forward nfe 14040, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 057, Runtime 0.442135, Loss 0.126821, forward nfe 14296, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 058, Runtime 0.420763, Loss 0.102239, forward nfe 14546, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 059, Runtime 0.382451, Loss 0.112990, forward nfe 14796, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 060, Runtime 0.412935, Loss 0.151307, forward nfe 15046, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 061, Runtime 0.399003, Loss 0.093187, forward nfe 15308, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 062, Runtime 0.435641, Loss 0.116626, forward nfe 15570, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 063, Runtime 0.437604, Loss 0.096686, forward nfe 15832, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 064, Runtime 0.443131, Loss 0.151915, forward nfe 16106, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 065, Runtime 0.446691, Loss 0.099965, forward nfe 16380, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 066, Runtime 0.454799, Loss 0.131248, forward nfe 16666, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 067, Runtime 0.449926, Loss 0.114946, forward nfe 16952, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 068, Runtime 0.433081, Loss 0.119894, forward nfe 17226, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 069, Runtime 0.378950, Loss 0.101228, forward nfe 17482, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 070, Runtime 0.438436, Loss 0.093400, forward nfe 17738, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 071, Runtime 0.407346, Loss 0.122549, forward nfe 18006, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 072, Runtime 0.405377, Loss 0.104368, forward nfe 18268, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 073, Runtime 0.445544, Loss 0.091151, forward nfe 18536, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 074, Runtime 0.454409, Loss 0.091521, forward nfe 18816, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 075, Runtime 0.450118, Loss 0.106860, forward nfe 19102, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 076, Runtime 0.445872, Loss 0.082081, forward nfe 19388, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 077, Runtime 0.448183, Loss 0.111162, forward nfe 19656, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 078, Runtime 0.445091, Loss 0.096267, forward nfe 19936, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 079, Runtime 0.451246, Loss 0.119163, forward nfe 20210, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 080, Runtime 0.444642, Loss 0.079809, forward nfe 20490, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 081, Runtime 0.459216, Loss 0.132604, forward nfe 20770, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 082, Runtime 0.449756, Loss 0.097280, forward nfe 21056, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 083, Runtime 0.446530, Loss 0.064737, forward nfe 21342, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 084, Runtime 0.449015, Loss 0.111731, forward nfe 21616, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 085, Runtime 0.452059, Loss 0.098091, forward nfe 21896, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 086, Runtime 0.450822, Loss 0.090878, forward nfe 22182, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 087, Runtime 0.447349, Loss 0.104149, forward nfe 22456, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 088, Runtime 0.442622, Loss 0.124214, forward nfe 22730, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 089, Runtime 0.446196, Loss 0.099359, forward nfe 23004, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 090, Runtime 0.446135, Loss 0.111293, forward nfe 23284, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 091, Runtime 0.452470, Loss 0.084105, forward nfe 23558, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 092, Runtime 0.462192, Loss 0.121389, forward nfe 23844, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 093, Runtime 0.452899, Loss 0.133933, forward nfe 24142, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 094, Runtime 0.456275, Loss 0.107376, forward nfe 24440, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 095, Runtime 0.453002, Loss 0.111691, forward nfe 24738, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 096, Runtime 0.459240, Loss 0.106065, forward nfe 25030, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 097, Runtime 0.457525, Loss 0.108179, forward nfe 25328, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 098, Runtime 0.459312, Loss 0.104346, forward nfe 25626, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "Epoch: 099, Runtime 0.479176, Loss 0.115829, forward nfe 25930, backward nfe 0, Train: 0.9286, Val: 0.8390, Test: 0.8402, Best time: 18.0000\n",
      "best val accuracy 0.838971 with test accuracy 0.840232 at epoch 7 and best time 18.000000\n"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'ACMP'\n",
    "opt['method'] = 'dopri5'\n",
    "opt['time'] = 18\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even when T = 50, ACMP is not suffer with oversommthing problem and maintain a good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using Cora dataset\n",
      "Epoch: 001, Runtime 0.608459, Loss 1.947030, forward nfe 116, backward nfe 0, Train: 0.2500, Val: 0.1860, Test: 0.1921, Best time: 0.0958\n",
      "Epoch: 002, Runtime 0.643505, Loss 1.826324, forward nfe 522, backward nfe 0, Train: 0.8286, Val: 0.7985, Test: 0.7724, Best time: 10.8699\n",
      "Epoch: 003, Runtime 0.715671, Loss 1.582338, forward nfe 958, backward nfe 0, Train: 0.8286, Val: 0.7985, Test: 0.7724, Best time: 50.0000\n",
      "Epoch: 004, Runtime 0.740445, Loss 1.353877, forward nfe 1436, backward nfe 0, Train: 0.8286, Val: 0.7985, Test: 0.7724, Best time: 50.0000\n",
      "Epoch: 005, Runtime 0.861322, Loss 1.107271, forward nfe 1956, backward nfe 0, Train: 0.8286, Val: 0.7985, Test: 0.7724, Best time: 50.0000\n",
      "Epoch: 006, Runtime 0.919091, Loss 0.968606, forward nfe 2524, backward nfe 0, Train: 0.9071, Val: 0.8316, Test: 0.7964, Best time: 21.4227\n",
      "Epoch: 007, Runtime 0.966079, Loss 0.746226, forward nfe 3140, backward nfe 0, Train: 0.9071, Val: 0.8316, Test: 0.7964, Best time: 50.0000\n",
      "Epoch: 008, Runtime 1.003627, Loss 0.639301, forward nfe 3792, backward nfe 0, Train: 0.9071, Val: 0.8316, Test: 0.7964, Best time: 50.0000\n",
      "Epoch: 009, Runtime 0.999411, Loss 0.506191, forward nfe 4462, backward nfe 0, Train: 0.9071, Val: 0.8316, Test: 0.7964, Best time: 50.0000\n",
      "Epoch: 010, Runtime 1.005108, Loss 0.498690, forward nfe 5138, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 17.3766\n",
      "Epoch: 011, Runtime 1.039644, Loss 0.488662, forward nfe 5838, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 012, Runtime 1.045714, Loss 0.464999, forward nfe 6538, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 013, Runtime 1.024314, Loss 0.465695, forward nfe 7238, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 014, Runtime 0.982352, Loss 0.409786, forward nfe 7914, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 015, Runtime 1.000140, Loss 0.445779, forward nfe 8566, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 016, Runtime 0.939002, Loss 0.450035, forward nfe 9218, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 017, Runtime 0.925351, Loss 0.468946, forward nfe 9840, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 018, Runtime 0.918602, Loss 0.413891, forward nfe 10462, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 019, Runtime 0.947945, Loss 0.369784, forward nfe 11090, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 020, Runtime 0.915042, Loss 0.452674, forward nfe 11718, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 021, Runtime 0.927606, Loss 0.404040, forward nfe 12334, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 022, Runtime 0.881236, Loss 0.355034, forward nfe 12956, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 023, Runtime 0.900648, Loss 0.389111, forward nfe 13572, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 024, Runtime 0.901916, Loss 0.402319, forward nfe 14176, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 025, Runtime 0.933530, Loss 0.340965, forward nfe 14774, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 026, Runtime 0.903887, Loss 0.388795, forward nfe 15402, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 027, Runtime 0.907513, Loss 0.392425, forward nfe 16012, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 028, Runtime 0.901099, Loss 0.436292, forward nfe 16634, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 029, Runtime 0.584750, Loss 0.399508, forward nfe 17250, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 030, Runtime 0.519596, Loss 0.291205, forward nfe 17866, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 031, Runtime 0.504542, Loss 0.354286, forward nfe 18488, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 032, Runtime 0.490738, Loss 0.424292, forward nfe 19080, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 033, Runtime 0.510661, Loss 0.359954, forward nfe 19666, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 034, Runtime 0.514975, Loss 0.371563, forward nfe 20264, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 035, Runtime 0.521585, Loss 0.311722, forward nfe 20886, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 036, Runtime 0.539224, Loss 0.355858, forward nfe 21526, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 037, Runtime 0.517336, Loss 0.320648, forward nfe 22166, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 038, Runtime 0.492599, Loss 0.255933, forward nfe 22782, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 039, Runtime 0.497256, Loss 0.277253, forward nfe 23374, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 040, Runtime 0.495541, Loss 0.339141, forward nfe 23966, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 041, Runtime 0.504156, Loss 0.234984, forward nfe 24540, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 042, Runtime 0.509854, Loss 0.306534, forward nfe 25138, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 043, Runtime 0.511183, Loss 0.335663, forward nfe 25742, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 044, Runtime 0.536095, Loss 0.329096, forward nfe 26370, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 045, Runtime 0.527848, Loss 0.316189, forward nfe 27004, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 046, Runtime 0.535959, Loss 0.304899, forward nfe 27632, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 047, Runtime 0.545982, Loss 0.344686, forward nfe 28278, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 048, Runtime 0.555639, Loss 0.258796, forward nfe 28936, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 049, Runtime 0.526965, Loss 0.326816, forward nfe 29594, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 050, Runtime 0.531044, Loss 0.299683, forward nfe 30234, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 051, Runtime 0.532197, Loss 0.254271, forward nfe 30862, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 052, Runtime 0.619460, Loss 0.290537, forward nfe 31484, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 053, Runtime 0.887390, Loss 0.276238, forward nfe 32136, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 054, Runtime 0.994798, Loss 0.256828, forward nfe 32794, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 055, Runtime 1.045994, Loss 0.249627, forward nfe 33470, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 056, Runtime 1.096207, Loss 0.254581, forward nfe 34182, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 057, Runtime 1.081796, Loss 0.311529, forward nfe 34930, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 058, Runtime 1.076078, Loss 0.230986, forward nfe 35672, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 059, Runtime 1.082653, Loss 0.267599, forward nfe 36414, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 060, Runtime 1.090630, Loss 0.248134, forward nfe 37162, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 061, Runtime 1.124645, Loss 0.269170, forward nfe 37916, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 062, Runtime 1.104612, Loss 0.301137, forward nfe 38688, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 063, Runtime 1.077513, Loss 0.218140, forward nfe 39424, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 064, Runtime 1.078030, Loss 0.256038, forward nfe 40148, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 065, Runtime 1.086069, Loss 0.250099, forward nfe 40884, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 066, Runtime 1.081423, Loss 0.203089, forward nfe 41626, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 067, Runtime 1.046011, Loss 0.215308, forward nfe 42368, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 068, Runtime 1.016029, Loss 0.232513, forward nfe 43074, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 069, Runtime 0.999100, Loss 0.301563, forward nfe 43768, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 070, Runtime 1.038760, Loss 0.223407, forward nfe 44474, backward nfe 0, Train: 0.9143, Val: 0.8382, Test: 0.8055, Best time: 50.0000\n",
      "Epoch: 071, Runtime 1.020076, Loss 0.230590, forward nfe 45186, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 109.1246\n",
      "Epoch: 072, Runtime 0.975765, Loss 0.247067, forward nfe 45886, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 073, Runtime 0.988247, Loss 0.261311, forward nfe 46550, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 074, Runtime 0.992734, Loss 0.357336, forward nfe 47214, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 075, Runtime 0.972322, Loss 0.303866, forward nfe 47896, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 076, Runtime 0.990619, Loss 0.258382, forward nfe 48554, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 077, Runtime 0.990752, Loss 0.295496, forward nfe 49230, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 078, Runtime 1.058528, Loss 0.274431, forward nfe 49912, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 079, Runtime 1.038448, Loss 0.356476, forward nfe 50636, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 080, Runtime 1.002028, Loss 0.276764, forward nfe 51348, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 081, Runtime 0.977354, Loss 0.282392, forward nfe 52006, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 082, Runtime 0.993694, Loss 0.254994, forward nfe 52670, backward nfe 0, Train: 0.9429, Val: 0.8412, Test: 0.8146, Best time: 50.0000\n",
      "Epoch: 083, Runtime 0.988486, Loss 0.281922, forward nfe 53370, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 28.6252\n",
      "Epoch: 084, Runtime 1.031570, Loss 0.257257, forward nfe 54052, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 085, Runtime 1.018685, Loss 0.267099, forward nfe 54752, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 086, Runtime 1.078998, Loss 0.303534, forward nfe 55476, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 087, Runtime 0.730386, Loss 0.224419, forward nfe 56224, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 088, Runtime 0.604613, Loss 0.271529, forward nfe 56966, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 089, Runtime 0.606778, Loss 0.221160, forward nfe 57708, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 090, Runtime 0.604784, Loss 0.246954, forward nfe 58462, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 091, Runtime 0.585679, Loss 0.213395, forward nfe 59204, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 092, Runtime 0.575057, Loss 0.210869, forward nfe 59916, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 093, Runtime 0.565799, Loss 0.223092, forward nfe 60628, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 094, Runtime 0.559063, Loss 0.179372, forward nfe 61334, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 095, Runtime 0.570823, Loss 0.228094, forward nfe 62022, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 096, Runtime 0.573958, Loss 0.209923, forward nfe 62728, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 097, Runtime 0.601447, Loss 0.243126, forward nfe 63446, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 098, Runtime 0.618261, Loss 0.192481, forward nfe 64188, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "Epoch: 099, Runtime 0.616963, Loss 0.196328, forward nfe 64960, backward nfe 0, Train: 0.9786, Val: 0.8426, Test: 0.8121, Best time: 50.0000\n",
      "best val accuracy 0.842647 with test accuracy 0.812086 at epoch 83 and best time 50.000000\n"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'ACMP'\n",
    "opt['method'] = 'dopri5'\n",
    "opt['time'] = 50\n",
    "_,_,test_acc,init_state,final_state = main(opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterophilic Dataset\n",
    "\n",
    "Texas homophily level: 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using GCN\n",
      "using GCN\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.439576, Loss 8.544854, forward nfe 86, backward nfe 0, Train: 0.3793, Val: 0.1695, Test: 0.1892, Best time: 2.1358\n",
      "Epoch: 002, Runtime 0.483616, Loss 82.503036, forward nfe 456, backward nfe 0, Train: 0.5172, Val: 0.4915, Test: 0.6216, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.510602, Loss 115.690140, forward nfe 838, backward nfe 0, Train: 0.5172, Val: 0.4915, Test: 0.6216, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.507909, Loss 91.526627, forward nfe 1220, backward nfe 0, Train: 0.5172, Val: 0.4915, Test: 0.6216, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.507847, Loss 45.897354, forward nfe 1602, backward nfe 0, Train: 0.5172, Val: 0.4915, Test: 0.6216, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.505346, Loss 17.459764, forward nfe 1984, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 0.0392\n",
      "Epoch: 007, Runtime 0.504986, Loss 4.895363, forward nfe 2366, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.507016, Loss 2.835251, forward nfe 2754, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.512077, Loss 9.524363, forward nfe 3142, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.492468, Loss 8.878380, forward nfe 3518, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.509399, Loss 5.128334, forward nfe 3906, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.523067, Loss 3.378338, forward nfe 4294, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.499256, Loss 2.251938, forward nfe 4682, backward nfe 0, Train: 0.7011, Val: 0.5424, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.513038, Loss 1.460325, forward nfe 5070, backward nfe 0, Train: 0.7816, Val: 0.5593, Test: 0.5405, Best time: 0.0374\n",
      "Epoch: 015, Runtime 0.501357, Loss 1.330684, forward nfe 5464, backward nfe 0, Train: 0.7816, Val: 0.5593, Test: 0.5405, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.489404, Loss 1.282432, forward nfe 5858, backward nfe 0, Train: 0.6897, Val: 0.5763, Test: 0.6757, Best time: 0.3123\n",
      "Epoch: 017, Runtime 0.485392, Loss 1.335487, forward nfe 6252, backward nfe 0, Train: 0.6897, Val: 0.5763, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.501617, Loss 0.922462, forward nfe 6646, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 3.0996\n",
      "Epoch: 019, Runtime 0.479067, Loss 1.113817, forward nfe 7040, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.510050, Loss 0.994960, forward nfe 7428, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.505880, Loss 1.038217, forward nfe 7816, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.510400, Loss 0.900155, forward nfe 8204, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.509922, Loss 0.706635, forward nfe 8592, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.506275, Loss 0.906246, forward nfe 8986, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.505875, Loss 0.814461, forward nfe 9380, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.516367, Loss 0.793318, forward nfe 9774, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.501240, Loss 0.878708, forward nfe 10168, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.331209, Loss 0.696087, forward nfe 10562, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.264805, Loss 0.837055, forward nfe 10956, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.268960, Loss 0.862381, forward nfe 11350, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.266159, Loss 0.737577, forward nfe 11744, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.270918, Loss 0.718017, forward nfe 12138, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.261211, Loss 0.689267, forward nfe 12532, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.260788, Loss 0.738008, forward nfe 12926, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.259593, Loss 1.069221, forward nfe 13320, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.257672, Loss 0.641263, forward nfe 13714, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.260279, Loss 0.793690, forward nfe 14108, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.264972, Loss 0.676792, forward nfe 14502, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.260729, Loss 0.775738, forward nfe 14896, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.261817, Loss 0.554527, forward nfe 15290, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.280962, Loss 0.872419, forward nfe 15684, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.267722, Loss 0.719612, forward nfe 16078, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.284708, Loss 0.620955, forward nfe 16472, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.281167, Loss 0.626335, forward nfe 16866, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.275684, Loss 0.616676, forward nfe 17260, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.295772, Loss 0.601057, forward nfe 17654, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.274657, Loss 0.599038, forward nfe 18048, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.277579, Loss 0.716706, forward nfe 18442, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.271266, Loss 0.600534, forward nfe 18836, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 050, Runtime 0.325191, Loss 0.544846, forward nfe 19230, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 051, Runtime 0.269445, Loss 0.547523, forward nfe 19624, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 052, Runtime 0.306761, Loss 0.540363, forward nfe 20018, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 053, Runtime 0.321088, Loss 0.477258, forward nfe 20412, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 054, Runtime 0.288857, Loss 0.570925, forward nfe 20806, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 055, Runtime 0.304818, Loss 0.518467, forward nfe 21200, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 056, Runtime 0.276131, Loss 0.605762, forward nfe 21594, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 057, Runtime 0.264030, Loss 0.587963, forward nfe 21988, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 058, Runtime 0.265421, Loss 0.539832, forward nfe 22382, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 059, Runtime 0.272250, Loss 0.495789, forward nfe 22776, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 060, Runtime 0.265331, Loss 0.521274, forward nfe 23170, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 061, Runtime 0.261859, Loss 0.604572, forward nfe 23564, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 062, Runtime 0.276217, Loss 0.490373, forward nfe 23958, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 063, Runtime 0.263604, Loss 0.445189, forward nfe 24352, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 064, Runtime 0.271829, Loss 0.486086, forward nfe 24746, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 065, Runtime 0.260650, Loss 0.477509, forward nfe 25140, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 066, Runtime 0.275989, Loss 0.457061, forward nfe 25534, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 067, Runtime 0.270738, Loss 0.444479, forward nfe 25928, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 068, Runtime 0.274101, Loss 0.496979, forward nfe 26322, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 069, Runtime 0.218227, Loss 0.442064, forward nfe 26716, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 070, Runtime 0.279050, Loss 0.443863, forward nfe 27110, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 071, Runtime 0.281859, Loss 0.445129, forward nfe 27504, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 072, Runtime 0.355026, Loss 0.460320, forward nfe 27898, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 073, Runtime 0.289611, Loss 0.416026, forward nfe 28292, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 074, Runtime 0.490887, Loss 0.465025, forward nfe 28686, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 075, Runtime 0.516962, Loss 0.492914, forward nfe 29080, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 076, Runtime 0.496774, Loss 0.516460, forward nfe 29474, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 077, Runtime 0.515139, Loss 0.482724, forward nfe 29868, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 078, Runtime 0.510601, Loss 0.520867, forward nfe 30262, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 079, Runtime 0.506634, Loss 0.453498, forward nfe 30656, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 080, Runtime 0.502380, Loss 0.460288, forward nfe 31050, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 081, Runtime 0.503616, Loss 0.385167, forward nfe 31444, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 082, Runtime 0.504026, Loss 0.432653, forward nfe 31838, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 083, Runtime 0.516160, Loss 0.407244, forward nfe 32232, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 084, Runtime 0.501436, Loss 0.442458, forward nfe 32626, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 085, Runtime 0.509934, Loss 0.426787, forward nfe 33020, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 086, Runtime 0.506857, Loss 0.364807, forward nfe 33414, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 087, Runtime 0.503674, Loss 0.442654, forward nfe 33808, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 088, Runtime 0.507603, Loss 0.429402, forward nfe 34202, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 089, Runtime 0.500647, Loss 0.432465, forward nfe 34596, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 090, Runtime 0.518615, Loss 0.982556, forward nfe 34990, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 091, Runtime 0.491576, Loss 0.432862, forward nfe 35384, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 092, Runtime 0.515449, Loss 0.399264, forward nfe 35778, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 093, Runtime 0.496535, Loss 0.359478, forward nfe 36172, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 094, Runtime 0.512793, Loss 0.415905, forward nfe 36566, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 095, Runtime 0.506478, Loss 0.467133, forward nfe 36960, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 096, Runtime 0.507831, Loss 0.588893, forward nfe 37354, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 097, Runtime 0.508032, Loss 0.511781, forward nfe 37748, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 098, Runtime 0.514011, Loss 0.409422, forward nfe 38142, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 099, Runtime 0.516167, Loss 0.452709, forward nfe 38536, backward nfe 0, Train: 0.7126, Val: 0.6271, Test: 0.6757, Best time: 5.0000\n",
      "best val accuracy 0.627119 with test accuracy 0.675676 at epoch 18 and best time 5.000000\n"
     ]
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "\n",
    "opt['geom_gcn_splits'] = True\n",
    "\n",
    "opt['function'] = 'GCN'\n",
    "\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.0\n",
    "\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.109777, Loss 1.627865, forward nfe 38, backward nfe 0, Train: 0.8736, Val: 0.7627, Test: 0.8378, Best time: 3.2194\n",
      "Epoch: 002, Runtime 0.119359, Loss 1.484387, forward nfe 156, backward nfe 0, Train: 0.8736, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.117156, Loss 1.219113, forward nfe 286, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 2.7987\n",
      "Epoch: 004, Runtime 0.125548, Loss 0.976388, forward nfe 416, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.138546, Loss 0.713359, forward nfe 546, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.121894, Loss 0.548220, forward nfe 682, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.122652, Loss 0.456125, forward nfe 818, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.132190, Loss 0.346771, forward nfe 954, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.120837, Loss 0.297650, forward nfe 1090, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.118173, Loss 0.233473, forward nfe 1226, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.116627, Loss 0.266238, forward nfe 1356, backward nfe 0, Train: 0.8506, Val: 0.7797, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.132324, Loss 0.162273, forward nfe 1486, backward nfe 0, Train: 0.9080, Val: 0.7966, Test: 0.7568, Best time: 0.6019\n",
      "Epoch: 013, Runtime 0.130474, Loss 0.151276, forward nfe 1616, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 2.7346\n",
      "Epoch: 014, Runtime 0.120827, Loss 0.122921, forward nfe 1746, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.129210, Loss 0.114727, forward nfe 1876, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.117171, Loss 0.116983, forward nfe 2006, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.121718, Loss 0.113295, forward nfe 2136, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.121215, Loss 0.102857, forward nfe 2266, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.119252, Loss 0.082501, forward nfe 2390, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.107592, Loss 0.113238, forward nfe 2514, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.114967, Loss 0.103304, forward nfe 2632, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.115342, Loss 0.107699, forward nfe 2750, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.118754, Loss 0.106904, forward nfe 2868, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.111113, Loss 0.072519, forward nfe 2986, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.114965, Loss 0.084972, forward nfe 3104, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.112814, Loss 0.091771, forward nfe 3222, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.116632, Loss 0.111682, forward nfe 3340, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.112634, Loss 0.075136, forward nfe 3458, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.111886, Loss 0.084194, forward nfe 3576, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.116586, Loss 0.070296, forward nfe 3694, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.114873, Loss 0.074894, forward nfe 3812, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.115785, Loss 0.069553, forward nfe 3930, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.106765, Loss 0.065038, forward nfe 4042, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.128371, Loss 0.074509, forward nfe 4154, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.118211, Loss 0.063749, forward nfe 4266, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.109168, Loss 0.067248, forward nfe 4378, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.105168, Loss 0.062757, forward nfe 4490, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.112440, Loss 0.048381, forward nfe 4602, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.113218, Loss 0.072881, forward nfe 4714, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.102636, Loss 0.052522, forward nfe 4826, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.105171, Loss 0.127737, forward nfe 4938, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.115158, Loss 0.046696, forward nfe 5050, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.112487, Loss 0.093600, forward nfe 5156, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.105963, Loss 0.077899, forward nfe 5262, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.110544, Loss 0.063705, forward nfe 5374, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.103125, Loss 0.079764, forward nfe 5480, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.104019, Loss 0.061902, forward nfe 5586, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.099433, Loss 0.062557, forward nfe 5692, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.108463, Loss 0.067827, forward nfe 5798, backward nfe 0, Train: 0.9885, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 050, Runtime 0.108461, Loss 0.046424, forward nfe 5904, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 051, Runtime 0.100942, Loss 0.073039, forward nfe 6010, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 052, Runtime 0.107415, Loss 0.052773, forward nfe 6116, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 053, Runtime 0.107475, Loss 0.057662, forward nfe 6222, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 054, Runtime 0.104167, Loss 0.064711, forward nfe 6328, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 055, Runtime 0.106217, Loss 0.045613, forward nfe 6434, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 056, Runtime 0.104270, Loss 0.070006, forward nfe 6540, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 057, Runtime 0.101977, Loss 0.046818, forward nfe 6646, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 058, Runtime 0.103522, Loss 0.039991, forward nfe 6752, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 059, Runtime 0.099912, Loss 0.039900, forward nfe 6858, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 060, Runtime 0.104982, Loss 0.043502, forward nfe 6964, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 061, Runtime 0.109127, Loss 0.076744, forward nfe 7070, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 062, Runtime 0.104409, Loss 0.031830, forward nfe 7176, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 063, Runtime 0.103384, Loss 0.070169, forward nfe 7282, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 064, Runtime 0.115716, Loss 0.043449, forward nfe 7388, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 065, Runtime 0.110066, Loss 0.051677, forward nfe 7494, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 066, Runtime 0.115670, Loss 0.053727, forward nfe 7600, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 067, Runtime 0.104882, Loss 0.053116, forward nfe 7706, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 068, Runtime 0.126319, Loss 0.045113, forward nfe 7812, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 069, Runtime 0.120185, Loss 0.037720, forward nfe 7918, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 070, Runtime 0.116545, Loss 0.052615, forward nfe 8024, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 071, Runtime 0.120478, Loss 0.037604, forward nfe 8130, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 072, Runtime 0.108546, Loss 0.050704, forward nfe 8236, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 073, Runtime 0.101474, Loss 0.064014, forward nfe 8342, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 074, Runtime 0.106390, Loss 0.078928, forward nfe 8448, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 075, Runtime 0.106155, Loss 0.068425, forward nfe 8554, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 076, Runtime 0.106927, Loss 0.076302, forward nfe 8660, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 077, Runtime 0.085960, Loss 0.050320, forward nfe 8766, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 078, Runtime 0.082940, Loss 0.055589, forward nfe 8872, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 079, Runtime 0.168888, Loss 0.053701, forward nfe 8978, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 080, Runtime 0.085193, Loss 0.047043, forward nfe 9084, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 081, Runtime 0.083532, Loss 0.052079, forward nfe 9190, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 082, Runtime 0.177012, Loss 0.042032, forward nfe 9296, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 083, Runtime 0.088105, Loss 0.048356, forward nfe 9402, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 084, Runtime 0.177888, Loss 0.044125, forward nfe 9508, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 085, Runtime 0.085693, Loss 0.090881, forward nfe 9614, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 086, Runtime 0.130389, Loss 0.056402, forward nfe 9720, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 2.7876\n",
      "Epoch: 087, Runtime 0.133801, Loss 0.043493, forward nfe 9826, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 088, Runtime 0.080085, Loss 0.037039, forward nfe 9926, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 089, Runtime 0.207722, Loss 0.055443, forward nfe 10026, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 090, Runtime 0.171911, Loss 0.042831, forward nfe 10126, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 091, Runtime 0.172592, Loss 0.042100, forward nfe 10226, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 092, Runtime 0.168946, Loss 0.033180, forward nfe 10326, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 093, Runtime 0.171219, Loss 0.043247, forward nfe 10426, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 094, Runtime 0.170500, Loss 0.042921, forward nfe 10526, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 095, Runtime 0.174984, Loss 0.037653, forward nfe 10626, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 096, Runtime 0.171367, Loss 0.030828, forward nfe 10726, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 097, Runtime 0.169910, Loss 0.035306, forward nfe 10826, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 098, Runtime 0.181483, Loss 0.035163, forward nfe 10926, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 099, Runtime 0.170573, Loss 0.036473, forward nfe 11026, backward nfe 0, Train: 1.0000, Val: 0.9153, Test: 0.8649, Best time: 5.0000\n",
      "best val accuracy 0.915254 with test accuracy 0.864865 at epoch 86 and best time 5.000000\n"
     ]
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "# opt['not_lcc'] = 'False'\n",
    "opt['function'] = 'ACMP'\n",
    "\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.5 #0.0 to 0.5\n",
    "\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$ ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.183041, Loss 1.662452, forward nfe 32, backward nfe 0, Train: 0.6322, Val: 0.5593, Test: 0.7027, Best time: 1.8629\n",
      "Epoch: 002, Runtime 0.196909, Loss 1.396572, forward nfe 138, backward nfe 0, Train: 0.6322, Val: 0.5593, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.255645, Loss 1.132694, forward nfe 274, backward nfe 0, Train: 0.6322, Val: 0.5593, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.196516, Loss 0.959510, forward nfe 422, backward nfe 0, Train: 0.8621, Val: 0.5932, Test: 0.6757, Best time: 0.0497\n",
      "Epoch: 005, Runtime 0.252304, Loss 0.798165, forward nfe 576, backward nfe 0, Train: 0.8621, Val: 0.5932, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.252496, Loss 0.628993, forward nfe 730, backward nfe 0, Train: 0.8621, Val: 0.5932, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.258773, Loss 0.480841, forward nfe 890, backward nfe 0, Train: 0.8621, Val: 0.5932, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.260752, Loss 0.460204, forward nfe 1050, backward nfe 0, Train: 0.8621, Val: 0.5932, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.260705, Loss 0.368121, forward nfe 1216, backward nfe 0, Train: 0.9080, Val: 0.6271, Test: 0.7027, Best time: 0.0489\n",
      "Epoch: 010, Runtime 0.258783, Loss 0.227146, forward nfe 1382, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 0.0490\n",
      "Epoch: 011, Runtime 0.259426, Loss 0.289147, forward nfe 1548, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.258855, Loss 0.248691, forward nfe 1714, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.257971, Loss 0.194314, forward nfe 1880, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.258746, Loss 0.165782, forward nfe 2046, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.261969, Loss 0.202424, forward nfe 2218, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.260189, Loss 0.216706, forward nfe 2390, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.263129, Loss 0.170672, forward nfe 2562, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.258822, Loss 0.121569, forward nfe 2734, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.282859, Loss 0.128027, forward nfe 2906, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.188716, Loss 0.091274, forward nfe 3090, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.172568, Loss 0.108760, forward nfe 3280, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.156771, Loss 0.154266, forward nfe 3464, backward nfe 0, Train: 0.9310, Val: 0.6610, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.157235, Loss 0.153989, forward nfe 3648, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 0.0516\n",
      "Epoch: 024, Runtime 0.176698, Loss 0.095785, forward nfe 3838, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.177308, Loss 0.141061, forward nfe 4034, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.168300, Loss 0.070466, forward nfe 4230, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.170209, Loss 0.110576, forward nfe 4414, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.160055, Loss 0.142311, forward nfe 4604, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.171510, Loss 0.070219, forward nfe 4800, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.165676, Loss 0.101973, forward nfe 4984, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.165165, Loss 0.082278, forward nfe 5168, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.167081, Loss 0.097221, forward nfe 5346, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.169830, Loss 0.125610, forward nfe 5542, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.156867, Loss 0.112147, forward nfe 5732, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.164640, Loss 0.175354, forward nfe 5922, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.153298, Loss 0.148433, forward nfe 6106, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.169656, Loss 0.101947, forward nfe 6302, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.151340, Loss 0.112218, forward nfe 6492, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.162830, Loss 0.122520, forward nfe 6676, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.154608, Loss 0.087049, forward nfe 6860, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.166135, Loss 0.086160, forward nfe 7050, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.167655, Loss 0.093136, forward nfe 7246, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.194388, Loss 0.101411, forward nfe 7442, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.169058, Loss 0.093614, forward nfe 7638, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.173161, Loss 0.078931, forward nfe 7834, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.171176, Loss 0.096563, forward nfe 8030, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.167489, Loss 0.063996, forward nfe 8226, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.162662, Loss 0.063457, forward nfe 8416, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.162448, Loss 0.103039, forward nfe 8612, backward nfe 0, Train: 0.9770, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "best val accuracy 0.677966 with test accuracy 0.729730 at epoch 23 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.076697, Loss 1.603943, forward nfe 32, backward nfe 0, Train: 0.5402, Val: 0.5254, Test: 0.6486, Best time: 0.0606\n",
      "Epoch: 002, Runtime 0.091459, Loss 1.371919, forward nfe 132, backward nfe 0, Train: 0.8736, Val: 0.6102, Test: 0.7027, Best time: 0.0565\n",
      "Epoch: 003, Runtime 0.100423, Loss 1.137598, forward nfe 238, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 0.5671\n",
      "Epoch: 004, Runtime 0.098956, Loss 0.873082, forward nfe 350, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.120291, Loss 0.684544, forward nfe 462, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.118941, Loss 0.548975, forward nfe 598, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.127390, Loss 0.403658, forward nfe 734, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.130873, Loss 0.321682, forward nfe 870, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.138511, Loss 0.266957, forward nfe 1018, backward nfe 0, Train: 0.8966, Val: 0.6780, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.141090, Loss 0.194442, forward nfe 1166, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 0.0528\n",
      "Epoch: 011, Runtime 0.146914, Loss 0.144215, forward nfe 1314, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.136541, Loss 0.141597, forward nfe 1462, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.124098, Loss 0.133518, forward nfe 1604, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.138058, Loss 0.096491, forward nfe 1752, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.138647, Loss 0.118327, forward nfe 1900, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.142202, Loss 0.078496, forward nfe 2048, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.129523, Loss 0.065190, forward nfe 2196, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.121778, Loss 0.087271, forward nfe 2344, backward nfe 0, Train: 0.9885, Val: 0.6949, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.142699, Loss 0.118248, forward nfe 2492, backward nfe 0, Train: 0.9885, Val: 0.7119, Test: 0.6757, Best time: 0.0554\n",
      "Epoch: 020, Runtime 0.126805, Loss 0.134519, forward nfe 2640, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 0.0558\n",
      "Epoch: 021, Runtime 0.142560, Loss 0.200068, forward nfe 2788, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.116530, Loss 0.164929, forward nfe 2930, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.128916, Loss 0.097321, forward nfe 3072, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.120360, Loss 0.096782, forward nfe 3214, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.122024, Loss 0.126546, forward nfe 3356, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.119758, Loss 0.112948, forward nfe 3498, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.133465, Loss 0.162972, forward nfe 3640, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.135434, Loss 0.105935, forward nfe 3788, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.132095, Loss 0.158096, forward nfe 3930, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.138124, Loss 0.098365, forward nfe 4078, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.126708, Loss 0.106726, forward nfe 4220, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.129454, Loss 0.109985, forward nfe 4356, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.123610, Loss 0.110009, forward nfe 4492, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.119530, Loss 0.105436, forward nfe 4628, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.121694, Loss 0.136552, forward nfe 4764, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.129985, Loss 0.146155, forward nfe 4900, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.122026, Loss 0.055330, forward nfe 5036, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.120994, Loss 0.093000, forward nfe 5172, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.133675, Loss 0.100023, forward nfe 5314, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.119865, Loss 0.109031, forward nfe 5450, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.130420, Loss 0.066149, forward nfe 5586, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.121623, Loss 0.080751, forward nfe 5728, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.117862, Loss 0.108448, forward nfe 5864, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.118134, Loss 0.127370, forward nfe 6000, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.138114, Loss 0.111106, forward nfe 6136, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.131347, Loss 0.058327, forward nfe 6272, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.123124, Loss 0.090934, forward nfe 6414, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.153950, Loss 0.089872, forward nfe 6562, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.134016, Loss 0.072258, forward nfe 6710, backward nfe 0, Train: 0.9770, Val: 0.7458, Test: 0.6757, Best time: 5.0000\n",
      "best val accuracy 0.745763 with test accuracy 0.675676 at epoch 20 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.091796, Loss 1.629244, forward nfe 32, backward nfe 0, Train: 0.5977, Val: 0.5763, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 002, Runtime 0.090735, Loss 1.285672, forward nfe 132, backward nfe 0, Train: 0.5977, Val: 0.5763, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.076845, Loss 0.937207, forward nfe 232, backward nfe 0, Train: 0.8161, Val: 0.6610, Test: 0.7568, Best time: 0.6970\n",
      "Epoch: 004, Runtime 0.085521, Loss 0.651717, forward nfe 332, backward nfe 0, Train: 0.9195, Val: 0.6949, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.148745, Loss 0.478300, forward nfe 438, backward nfe 0, Train: 0.9195, Val: 0.6949, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.086882, Loss 0.324407, forward nfe 544, backward nfe 0, Train: 0.9195, Val: 0.6949, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.154223, Loss 0.201856, forward nfe 650, backward nfe 0, Train: 0.9885, Val: 0.7119, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.085264, Loss 0.166982, forward nfe 756, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.087616, Loss 0.139932, forward nfe 862, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.148290, Loss 0.094890, forward nfe 968, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.084636, Loss 0.091857, forward nfe 1074, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.084490, Loss 0.083792, forward nfe 1180, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.153290, Loss 0.096465, forward nfe 1286, backward nfe 0, Train: 1.0000, Val: 0.7458, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.086929, Loss 0.081778, forward nfe 1392, backward nfe 0, Train: 1.0000, Val: 0.7966, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.190119, Loss 0.043243, forward nfe 1504, backward nfe 0, Train: 1.0000, Val: 0.7966, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.175172, Loss 0.050431, forward nfe 1610, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 2.1100\n",
      "Epoch: 017, Runtime 0.185620, Loss 0.095871, forward nfe 1716, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.172385, Loss 0.062530, forward nfe 1822, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.184722, Loss 0.057806, forward nfe 1940, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.180654, Loss 0.085703, forward nfe 2058, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.178820, Loss 0.105337, forward nfe 2176, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.228983, Loss 0.122277, forward nfe 2294, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.179640, Loss 0.097343, forward nfe 2412, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.182209, Loss 0.119824, forward nfe 2530, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.175328, Loss 0.102570, forward nfe 2648, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.176824, Loss 0.133080, forward nfe 2766, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.175364, Loss 0.144896, forward nfe 2884, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.176224, Loss 0.078784, forward nfe 3002, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.177029, Loss 0.078772, forward nfe 3120, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.177480, Loss 0.090269, forward nfe 3238, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.214331, Loss 0.150234, forward nfe 3356, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.189876, Loss 0.097590, forward nfe 3474, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.183912, Loss 0.043069, forward nfe 3592, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.172773, Loss 0.081573, forward nfe 3704, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.177082, Loss 0.177395, forward nfe 3822, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.173752, Loss 0.069554, forward nfe 3934, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.171557, Loss 0.085635, forward nfe 4046, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.173663, Loss 0.078965, forward nfe 4164, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.178392, Loss 0.082244, forward nfe 4282, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.180725, Loss 0.059536, forward nfe 4394, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.229865, Loss 0.111580, forward nfe 4512, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.187505, Loss 0.082637, forward nfe 4630, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.175732, Loss 0.068868, forward nfe 4742, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.175611, Loss 0.101929, forward nfe 4860, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.185049, Loss 0.087211, forward nfe 4978, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.221911, Loss 0.066733, forward nfe 5096, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.184690, Loss 0.058688, forward nfe 5214, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.177727, Loss 0.086531, forward nfe 5332, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.174505, Loss 0.111140, forward nfe 5450, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "best val accuracy 0.813559 with test accuracy 0.783784 at epoch 16 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.162712, Loss 1.625225, forward nfe 32, backward nfe 0, Train: 0.7471, Val: 0.6271, Test: 0.7027, Best time: 4.4894\n",
      "Epoch: 002, Runtime 0.171437, Loss 1.276007, forward nfe 132, backward nfe 0, Train: 0.7816, Val: 0.6949, Test: 0.7297, Best time: 4.1095\n",
      "Epoch: 003, Runtime 0.172709, Loss 0.900978, forward nfe 238, backward nfe 0, Train: 0.8046, Val: 0.7119, Test: 0.7568, Best time: 2.7704\n",
      "Epoch: 004, Runtime 0.178624, Loss 0.642837, forward nfe 344, backward nfe 0, Train: 0.8046, Val: 0.7119, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.168943, Loss 0.539156, forward nfe 450, backward nfe 0, Train: 0.8046, Val: 0.7119, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.168980, Loss 0.355698, forward nfe 556, backward nfe 0, Train: 0.9080, Val: 0.7288, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.165767, Loss 0.289623, forward nfe 662, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.8108, Best time: 0.7200\n",
      "Epoch: 008, Runtime 0.159671, Loss 0.254847, forward nfe 768, backward nfe 0, Train: 0.9655, Val: 0.7797, Test: 0.8108, Best time: 2.8029\n",
      "Epoch: 009, Runtime 0.172679, Loss 0.195000, forward nfe 874, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 0.7272\n",
      "Epoch: 010, Runtime 0.172106, Loss 0.157970, forward nfe 980, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.172297, Loss 0.127785, forward nfe 1086, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.172619, Loss 0.128583, forward nfe 1192, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.173881, Loss 0.156510, forward nfe 1298, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.163789, Loss 0.104465, forward nfe 1404, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.172793, Loss 0.143387, forward nfe 1504, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.169972, Loss 0.100155, forward nfe 1604, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.172524, Loss 0.067211, forward nfe 1704, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.171551, Loss 0.065570, forward nfe 1804, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.169465, Loss 0.062535, forward nfe 1904, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.171923, Loss 0.088989, forward nfe 2004, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.171028, Loss 0.102721, forward nfe 2104, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.174244, Loss 0.066072, forward nfe 2204, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.169840, Loss 0.100647, forward nfe 2304, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.172867, Loss 0.103039, forward nfe 2404, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.171793, Loss 0.099758, forward nfe 2498, backward nfe 0, Train: 0.9770, Val: 0.8136, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.140372, Loss 0.107152, forward nfe 2592, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.170349, Loss 0.087177, forward nfe 2680, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.167209, Loss 0.071390, forward nfe 2768, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.164392, Loss 0.080663, forward nfe 2856, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.159501, Loss 0.106332, forward nfe 2944, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.133425, Loss 0.057441, forward nfe 3032, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.170724, Loss 0.038279, forward nfe 3120, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.165659, Loss 0.067242, forward nfe 3208, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.157075, Loss 0.062306, forward nfe 3296, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.160817, Loss 0.126752, forward nfe 3384, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.156419, Loss 0.057344, forward nfe 3472, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.110630, Loss 0.079593, forward nfe 3560, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.164741, Loss 0.081296, forward nfe 3648, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.158059, Loss 0.057144, forward nfe 3736, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.110024, Loss 0.071282, forward nfe 3824, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.164486, Loss 0.060737, forward nfe 3912, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.151068, Loss 0.088652, forward nfe 4000, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.138367, Loss 0.066478, forward nfe 4088, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.147057, Loss 0.047014, forward nfe 4176, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.159813, Loss 0.055479, forward nfe 4264, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.154004, Loss 0.097472, forward nfe 4352, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.143140, Loss 0.034731, forward nfe 4440, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.148272, Loss 0.062400, forward nfe 4528, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.168620, Loss 0.058492, forward nfe 4616, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.7838, Best time: 5.0000\n",
      "best val accuracy 0.847458 with test accuracy 0.783784 at epoch 26 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.178857, Loss 1.566180, forward nfe 32, backward nfe 0, Train: 0.7471, Val: 0.6441, Test: 0.7027, Best time: 3.7627\n",
      "Epoch: 002, Runtime 0.185882, Loss 1.417494, forward nfe 138, backward nfe 0, Train: 0.7356, Val: 0.6949, Test: 0.7838, Best time: 1.5281\n",
      "Epoch: 003, Runtime 0.186529, Loss 1.149799, forward nfe 256, backward nfe 0, Train: 0.7356, Val: 0.6949, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.179889, Loss 0.859893, forward nfe 374, backward nfe 0, Train: 0.7356, Val: 0.6949, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.177258, Loss 0.652075, forward nfe 492, backward nfe 0, Train: 0.7356, Val: 0.6949, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.202737, Loss 0.525171, forward nfe 616, backward nfe 0, Train: 0.8506, Val: 0.7119, Test: 0.7838, Best time: 1.3791\n",
      "Epoch: 007, Runtime 0.215450, Loss 0.480963, forward nfe 740, backward nfe 0, Train: 0.8736, Val: 0.7458, Test: 0.8378, Best time: 3.0767\n",
      "Epoch: 008, Runtime 0.182051, Loss 0.385532, forward nfe 864, backward nfe 0, Train: 0.8851, Val: 0.7627, Test: 0.8108, Best time: 1.4026\n",
      "Epoch: 009, Runtime 0.210247, Loss 0.293116, forward nfe 982, backward nfe 0, Train: 0.8736, Val: 0.7797, Test: 0.7838, Best time: 0.6661\n",
      "Epoch: 010, Runtime 0.195117, Loss 0.265306, forward nfe 1100, backward nfe 0, Train: 0.8736, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.184705, Loss 0.236576, forward nfe 1218, backward nfe 0, Train: 0.9310, Val: 0.7966, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.176443, Loss 0.218592, forward nfe 1336, backward nfe 0, Train: 0.9310, Val: 0.7966, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.176629, Loss 0.161083, forward nfe 1454, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.7568, Best time: 1.5115\n",
      "Epoch: 014, Runtime 0.177171, Loss 0.170988, forward nfe 1572, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.182300, Loss 0.119060, forward nfe 1690, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.228976, Loss 0.129950, forward nfe 1808, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.184415, Loss 0.117854, forward nfe 1926, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.178754, Loss 0.152463, forward nfe 2038, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.181370, Loss 0.152761, forward nfe 2150, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.220841, Loss 0.104073, forward nfe 2262, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.177781, Loss 0.094165, forward nfe 2374, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.189294, Loss 0.097149, forward nfe 2486, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.172842, Loss 0.086617, forward nfe 2592, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.170309, Loss 0.091045, forward nfe 2698, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.166758, Loss 0.089936, forward nfe 2804, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.161705, Loss 0.073146, forward nfe 2910, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.173760, Loss 0.079147, forward nfe 3016, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.174507, Loss 0.093727, forward nfe 3122, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.173979, Loss 0.073758, forward nfe 3228, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.175265, Loss 0.068188, forward nfe 3334, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.175801, Loss 0.059084, forward nfe 3440, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.162162, Loss 0.067933, forward nfe 3546, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.170549, Loss 0.093453, forward nfe 3646, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.172120, Loss 0.073457, forward nfe 3746, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.171361, Loss 0.087551, forward nfe 3846, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.171011, Loss 0.071724, forward nfe 3946, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.172976, Loss 0.061251, forward nfe 4046, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.174017, Loss 0.086989, forward nfe 4146, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.175425, Loss 0.058753, forward nfe 4246, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.173054, Loss 0.074706, forward nfe 4346, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.169173, Loss 0.069622, forward nfe 4446, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.171772, Loss 0.051950, forward nfe 4546, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.172126, Loss 0.072634, forward nfe 4646, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.170122, Loss 0.035771, forward nfe 4746, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.169905, Loss 0.034123, forward nfe 4846, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.170502, Loss 0.059684, forward nfe 4946, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.174761, Loss 0.077755, forward nfe 5046, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.172703, Loss 0.039525, forward nfe 5146, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.171024, Loss 0.059114, forward nfe 5246, backward nfe 0, Train: 0.9885, Val: 0.8644, Test: 0.8378, Best time: 5.0000\n",
      "best val accuracy 0.864407 with test accuracy 0.837838 at epoch 14 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using texas dataset\n",
      "Epoch: 001, Runtime 0.170844, Loss 1.627374, forward nfe 38, backward nfe 0, Train: 0.7011, Val: 0.6780, Test: 0.7297, Best time: 1.4394\n",
      "Epoch: 002, Runtime 0.292418, Loss 1.485866, forward nfe 156, backward nfe 0, Train: 0.7816, Val: 0.7288, Test: 0.7838, Best time: 2.0454\n",
      "Epoch: 003, Runtime 0.184224, Loss 1.245796, forward nfe 286, backward nfe 0, Train: 0.8161, Val: 0.7458, Test: 0.8108, Best time: 3.5669\n",
      "Epoch: 004, Runtime 0.244255, Loss 0.961234, forward nfe 416, backward nfe 0, Train: 0.8161, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.187177, Loss 0.711990, forward nfe 552, backward nfe 0, Train: 0.8161, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.245381, Loss 0.562556, forward nfe 688, backward nfe 0, Train: 0.8161, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.189094, Loss 0.470677, forward nfe 824, backward nfe 0, Train: 0.8161, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.246253, Loss 0.419059, forward nfe 960, backward nfe 0, Train: 0.8851, Val: 0.7627, Test: 0.8378, Best time: 3.3371\n",
      "Epoch: 009, Runtime 0.189946, Loss 0.362297, forward nfe 1096, backward nfe 0, Train: 0.8851, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.244306, Loss 0.271918, forward nfe 1232, backward nfe 0, Train: 0.9310, Val: 0.7797, Test: 0.8378, Best time: 2.6388\n",
      "Epoch: 011, Runtime 0.183407, Loss 0.227012, forward nfe 1368, backward nfe 0, Train: 0.9310, Val: 0.7797, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.231031, Loss 0.195045, forward nfe 1498, backward nfe 0, Train: 0.9310, Val: 0.7797, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.193186, Loss 0.169713, forward nfe 1628, backward nfe 0, Train: 0.9770, Val: 0.7966, Test: 0.8378, Best time: 4.6706\n",
      "Epoch: 014, Runtime 0.222598, Loss 0.147943, forward nfe 1758, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 1.9871\n",
      "Epoch: 015, Runtime 0.198381, Loss 0.146372, forward nfe 1888, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.188435, Loss 0.131378, forward nfe 2018, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.244282, Loss 0.150246, forward nfe 2148, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.184214, Loss 0.133231, forward nfe 2272, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.181217, Loss 0.124051, forward nfe 2396, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.232009, Loss 0.093478, forward nfe 2514, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.190053, Loss 0.092010, forward nfe 2632, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.179831, Loss 0.126641, forward nfe 2750, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.182902, Loss 0.109746, forward nfe 2868, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.198795, Loss 0.110483, forward nfe 2986, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.170020, Loss 0.107468, forward nfe 3104, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.219558, Loss 0.098456, forward nfe 3222, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.186698, Loss 0.069905, forward nfe 3340, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 3.3691\n",
      "Epoch: 028, Runtime 0.186170, Loss 0.087244, forward nfe 3458, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.174564, Loss 0.099781, forward nfe 3576, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.176549, Loss 0.078722, forward nfe 3688, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.180672, Loss 0.082516, forward nfe 3800, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.219016, Loss 0.109398, forward nfe 3912, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.173287, Loss 0.058890, forward nfe 4024, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.174200, Loss 0.087141, forward nfe 4136, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.175632, Loss 0.062816, forward nfe 4248, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.175772, Loss 0.064823, forward nfe 4360, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.173686, Loss 0.078199, forward nfe 4472, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.174567, Loss 0.079672, forward nfe 4584, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.174627, Loss 0.076853, forward nfe 4696, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.176562, Loss 0.081843, forward nfe 4808, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.174304, Loss 0.068661, forward nfe 4920, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.182947, Loss 0.061648, forward nfe 5032, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.166162, Loss 0.074123, forward nfe 5138, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.188444, Loss 0.053115, forward nfe 5250, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.174478, Loss 0.076224, forward nfe 5362, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.173129, Loss 0.062930, forward nfe 5474, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.174352, Loss 0.042678, forward nfe 5586, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.164788, Loss 0.053105, forward nfe 5692, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.162474, Loss 0.051392, forward nfe 5798, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "best val accuracy 0.864407 with test accuracy 0.864865 at epoch 27 and best time 5.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe163e83a30>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "\n",
    "opt['function'] = 'ACMP'\n",
    "opt['epoch'] = 50\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.5\n",
    "test_acc = []\n",
    "for beta in [0.0,0.1,0.2,0.3,0.4,0.5]:\n",
    "    opt['beta'] = beta\n",
    "    _,_,test_acc_,_,_ = main(opt)\n",
    "    test_acc.append(test_acc_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( [0.0,0.1,0.2,0.3,0.4,0.5],test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model and code is from twitter-research: https://github.com/twitter-research/graph-neural-pde\n",
    "\n",
    "\n",
    "\n",
    "Chamberlain, Ben, et al. \"Grand: Graph neural diffusion.\" International Conference on Machine Learning. PMLR, 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1ff16980caed4c53325d748eca9b3ef767713c55c7da06ab7bd73a6f59e9560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
