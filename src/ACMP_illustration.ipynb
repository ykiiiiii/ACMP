{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNN import GNN\n",
    "from run_GNN import get_optimizer, test,  train, main\n",
    "from best_params import best_params_dict\n",
    "from data import get_dataset\n",
    "from utils import ROOT_DIR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "opt = np.load('cora_arg.npy',allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "def plot_state(init_state,final_state):\n",
    "    fig, ax = plt.subplots(2)\n",
    "    ax[0].scatter(init_state[:,0].cpu().numpy(),init_state[:,1].cpu().numpy())\n",
    "    ax[1].scatter(final_state[:,0].cpu().numpy(),final_state[:,1].cpu().numpy())\n",
    "    ax[1].set_xlim(-3,3)\n",
    "    ax[1].set_ylim(-3,3)\n",
    "    ax[0].set_xlim(-3,3)\n",
    "    ax[0].set_ylim(-3,3)\n",
    "    # plt.xlim(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN +Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['channel_mixing'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'channel_mixing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/kai/ACMP/src/ACMP_illustration.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m opt[\u001b[39m'\u001b[39m\u001b[39mfunction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mGCN\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m opt[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m _,_,_,init_state,final_state \u001b[39m=\u001b[39m main(opt)\n",
      "File \u001b[0;32m~/ACMP/src/run_GNN.py:240\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cmd_opt)\u001b[0m\n\u001b[1;32m    238\u001b[0m   model \u001b[39m=\u001b[39m GNN_KNN(opt, dataset, device)\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m opt[\u001b[39m\"\u001b[39m\u001b[39mno_early\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m GNNKNNEarly(opt, dataset, device)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m   model \u001b[39m=\u001b[39m GNN(opt, dataset, device)\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m opt[\u001b[39m\"\u001b[39m\u001b[39mno_early\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m GNNEarly(opt, dataset, device)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m opt[\u001b[39m'\u001b[39m\u001b[39mplanetoid_split\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m opt[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mCora\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mCiteseer\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPubmed\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    243\u001b[0m   dataset\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m set_train_val_test_split(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1000\u001b[39m), dataset\u001b[39m.\u001b[39mdata, num_development\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m \u001b[39mif\u001b[39;00m opt[\u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCoauthorCS\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1500\u001b[39m)\n",
      "File \u001b[0;32m~/ACMP/src/GNN_early.py:25\u001b[0m, in \u001b[0;36mGNNEarly.__init__\u001b[0;34m(self, opt, dataset, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m time_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mT])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[39m# self.regularization_fns = ()\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39modeblock \u001b[39m=\u001b[39m block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregularization_fns, opt, dataset\u001b[39m.\u001b[39;49mdata, device, t\u001b[39m=\u001b[39;49mtime_tensor)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[39m# overwrite the test integrator with this custom one\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/ACMP/src/block_transformer_attention.py:9\u001b[0m, in \u001b[0;36mAttODEblock.__init__\u001b[0;34m(self, odefunc, regularization_fns, opt, data, device, t, gamma)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, odefunc, regularization_fns, opt, data, device, t\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]), gamma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m   \u001b[39msuper\u001b[39;49m(AttODEblock, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(odefunc, regularization_fns, opt, data, device, t)\n\u001b[1;32m     11\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39modefunc \u001b[39m=\u001b[39m odefunc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_dim \u001b[39m*\u001b[39m opt[\u001b[39m'\u001b[39m\u001b[39mhidden_dim\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_dim \u001b[39m*\u001b[39m opt[\u001b[39m'\u001b[39m\u001b[39mhidden_dim\u001b[39m\u001b[39m'\u001b[39m], opt, data, device)\n\u001b[1;32m     12\u001b[0m   \u001b[39m# self.odefunc.edge_index, self.odefunc.edge_weight = data.edge_index, edge_weight=data.edge_attr\u001b[39;00m\n",
      "File \u001b[0;32m~/ACMP/src/base_classes.py:39\u001b[0m, in \u001b[0;36mODEblock.__init__\u001b[0;34m(self, odefunc, regularization_fns, opt, data, device, t)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m=\u001b[39m t\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m opt[\u001b[39m'\u001b[39m\u001b[39maugment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39modefunc \u001b[39m=\u001b[39m odefunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maug_dim \u001b[39m*\u001b[39;49m opt[\u001b[39m'\u001b[39;49m\u001b[39mhidden_dim\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maug_dim \u001b[39m*\u001b[39;49m opt[\u001b[39m'\u001b[39;49m\u001b[39mhidden_dim\u001b[39;49m\u001b[39m'\u001b[39;49m], opt, data, device)\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnreg \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(regularization_fns)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_odefunc \u001b[39m=\u001b[39m RegularizedODEfunc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39modefunc, regularization_fns)\n",
      "File \u001b[0;32m~/ACMP/src/function_laplacian_diffusion_GCN.py:19\u001b[0m, in \u001b[0;36mLaplacianODEFunc.__init__\u001b[0;34m(self, in_features, out_features, opt, data, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_features, out_features, opt, data, device):\n\u001b[0;32m---> 19\u001b[0m   \u001b[39msuper\u001b[39;49m(LaplacianODEFunc, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(opt, data, device)\n\u001b[1;32m     20\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39musing GCN\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n",
      "File \u001b[0;32m~/ACMP/src/base_classes.py:87\u001b[0m, in \u001b[0;36mODEFunc.__init__\u001b[0;34m(self, opt, data, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_weight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mchannel_mixing\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[1;32m     88\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha_train \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(opt[\u001b[39m'\u001b[39m\u001b[39minit_alpha\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mones(opt[\u001b[39m'\u001b[39m\u001b[39mhidden_dim\u001b[39m\u001b[39m'\u001b[39m],opt[\u001b[39m'\u001b[39m\u001b[39mhidden_dim\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'channel_mixing'"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'GCN'\n",
    "opt['time'] = 3\n",
    "_,_,_,init_state,final_state = main(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversmoothing problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "using GCN\n",
      "using GCN\n",
      "using Cora dataset\n",
      "Epoch: 001, Runtime 0.246560, Loss 58083311422290264064.000000, forward nfe 200, backward nfe 0, Train: 0.2000, Val: 0.3382, Test: 0.3071, Best time: 4.0000\n",
      "Epoch: 002, Runtime 0.246150, Loss 1267229759286439575552.000000, forward nfe 1000, backward nfe 0, Train: 0.2000, Val: 0.3382, Test: 0.3071, Best time: 50.0000\n",
      "Epoch: 003, Runtime 0.243845, Loss 1712582563662783315968.000000, forward nfe 1800, backward nfe 0, Train: 0.2000, Val: 0.3382, Test: 0.3071, Best time: 50.0000\n",
      "Epoch: 004, Runtime 0.692312, Loss 1669345896177701224448.000000, forward nfe 2600, backward nfe 0, Train: 0.2000, Val: 0.3382, Test: 0.3071, Best time: 50.0000\n",
      "Epoch: 005, Runtime 0.243299, Loss 1507298359548074524672.000000, forward nfe 3400, backward nfe 0, Train: 0.3929, Val: 0.3735, Test: 0.3485, Best time: 1.0000\n",
      "Epoch: 006, Runtime 0.248328, Loss 1391849286787873636352.000000, forward nfe 4200, backward nfe 0, Train: 0.3929, Val: 0.3735, Test: 0.3485, Best time: 50.0000\n",
      "Epoch: 007, Runtime 0.243437, Loss 1247768157184199753728.000000, forward nfe 5000, backward nfe 0, Train: 0.3929, Val: 0.3735, Test: 0.3485, Best time: 50.0000\n",
      "Epoch: 008, Runtime 0.248253, Loss 817512474289964580864.000000, forward nfe 5800, backward nfe 0, Train: 0.4429, Val: 0.4140, Test: 0.3891, Best time: 1.0000\n",
      "Epoch: 009, Runtime 0.423599, Loss 483713938236704292864.000000, forward nfe 6600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 2.0000\n",
      "Epoch: 010, Runtime 0.462412, Loss 215997440795479113728.000000, forward nfe 7400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 011, Runtime 0.241435, Loss 177237755960282316800.000000, forward nfe 8200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 012, Runtime 0.240316, Loss 107452905233525506048.000000, forward nfe 9000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 013, Runtime 0.241310, Loss 32601949128674508800.000000, forward nfe 9800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 014, Runtime 0.245783, Loss 27061743132776333312.000000, forward nfe 10600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 015, Runtime 0.694043, Loss 19432627771824668672.000000, forward nfe 11400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 016, Runtime 0.243916, Loss 9679987121242243072.000000, forward nfe 12200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 017, Runtime 0.243480, Loss 2871835036431155200.000000, forward nfe 13000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 018, Runtime 0.245553, Loss 1604793158393135104.000000, forward nfe 13800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 019, Runtime 0.244191, Loss 1243903480465195008.000000, forward nfe 14600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 020, Runtime 0.611810, Loss 575026989140803584.000000, forward nfe 15400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 021, Runtime 0.297896, Loss 376872007150600192.000000, forward nfe 16200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 022, Runtime 0.245549, Loss 91321810850676736.000000, forward nfe 17000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 023, Runtime 0.242552, Loss 67562335237046272.000000, forward nfe 17800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 024, Runtime 0.242880, Loss 2138455895179264.000000, forward nfe 18600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 025, Runtime 0.245224, Loss 320994581413888.000000, forward nfe 19400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 026, Runtime 0.672796, Loss 1.798669, forward nfe 20200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 027, Runtime 0.251764, Loss 1.798611, forward nfe 21000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 028, Runtime 0.243101, Loss 1.798548, forward nfe 21800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 029, Runtime 0.245642, Loss 1.798481, forward nfe 22600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 030, Runtime 0.244095, Loss 1.827630, forward nfe 23400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 031, Runtime 0.361464, Loss 1.812945, forward nfe 24200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 032, Runtime 0.555874, Loss 1.827475, forward nfe 25000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 033, Runtime 0.245059, Loss 1.827393, forward nfe 25800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 034, Runtime 0.246372, Loss 1.841915, forward nfe 26600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 035, Runtime 0.245880, Loss 1.827218, forward nfe 27400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 036, Runtime 0.245705, Loss 1.827128, forward nfe 28200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 037, Runtime 0.559466, Loss 1.827034, forward nfe 29000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 038, Runtime 0.351234, Loss 1.841541, forward nfe 29800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 039, Runtime 0.240887, Loss 1.826843, forward nfe 30600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 040, Runtime 0.241050, Loss 1.841342, forward nfe 31400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 041, Runtime 0.243246, Loss 1.826645, forward nfe 32200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 042, Runtime 0.245321, Loss 1.841138, forward nfe 33000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 043, Runtime 0.706345, Loss 1.841034, forward nfe 33800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 044, Runtime 0.259771, Loss 1.826343, forward nfe 34600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 045, Runtime 0.250198, Loss 1.826241, forward nfe 35400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 046, Runtime 0.240501, Loss 1.826139, forward nfe 36200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 047, Runtime 0.252620, Loss 1.840613, forward nfe 37000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 048, Runtime 0.697445, Loss 1.825934, forward nfe 37800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 049, Runtime 0.310774, Loss 1.825831, forward nfe 38600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 050, Runtime 0.252502, Loss 1.825729, forward nfe 39400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 051, Runtime 0.253755, Loss 263323981061292032.000000, forward nfe 40200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 052, Runtime 0.250181, Loss 1.825526, forward nfe 41000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 053, Runtime 0.360121, Loss 1.825425, forward nfe 41800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 054, Runtime 0.562425, Loss 1.825325, forward nfe 42600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 055, Runtime 0.249957, Loss 1.825225, forward nfe 43400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 056, Runtime 0.253757, Loss 1.825126, forward nfe 44200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 057, Runtime 0.251396, Loss 1.839572, forward nfe 45000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 058, Runtime 0.250263, Loss 1.824928, forward nfe 45800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 059, Runtime 0.688592, Loss 1.824829, forward nfe 46600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 060, Runtime 0.258201, Loss 1.839266, forward nfe 47400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 061, Runtime 0.250643, Loss 1.824634, forward nfe 48200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 062, Runtime 0.254312, Loss 1.824536, forward nfe 49000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 063, Runtime 0.255065, Loss 1.824440, forward nfe 49800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 064, Runtime 0.711482, Loss 1.824344, forward nfe 50600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 065, Runtime 0.248507, Loss 1.824249, forward nfe 51400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 066, Runtime 0.246949, Loss 1.838670, forward nfe 52200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 067, Runtime 0.248837, Loss 1.824060, forward nfe 53000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 068, Runtime 0.253644, Loss 1.823966, forward nfe 53800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 069, Runtime 0.369690, Loss 1.823874, forward nfe 54600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 070, Runtime 0.550694, Loss 1.823781, forward nfe 55400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 071, Runtime 0.251338, Loss 1.823690, forward nfe 56200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 072, Runtime 0.248977, Loss 1.823599, forward nfe 57000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 073, Runtime 0.254597, Loss 1.823509, forward nfe 57800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 074, Runtime 0.246889, Loss 1.823419, forward nfe 58600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 075, Runtime 0.695498, Loss 1.837817, forward nfe 59400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 076, Runtime 0.247573, Loss 1.823243, forward nfe 60200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 077, Runtime 0.255507, Loss 1.823155, forward nfe 61000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 078, Runtime 0.250096, Loss 1.823068, forward nfe 61800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 079, Runtime 0.248030, Loss 1.822982, forward nfe 62600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 080, Runtime 0.656621, Loss 1.837367, forward nfe 63400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 081, Runtime 0.260560, Loss 1.837278, forward nfe 64200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 082, Runtime 0.252555, Loss 1.822726, forward nfe 65000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 083, Runtime 0.252991, Loss 1.837103, forward nfe 65800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 084, Runtime 0.251646, Loss 1.837017, forward nfe 66600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 085, Runtime 0.251840, Loss 1.822475, forward nfe 67400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 086, Runtime 0.675905, Loss 1.836845, forward nfe 68200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 087, Runtime 0.247784, Loss 1.836759, forward nfe 69000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 088, Runtime 0.250541, Loss 1.822230, forward nfe 69800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 089, Runtime 0.251591, Loss 1.822149, forward nfe 70600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 090, Runtime 0.249327, Loss 1.822070, forward nfe 71400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 091, Runtime 0.648255, Loss 1.821991, forward nfe 72200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 092, Runtime 0.269085, Loss 1.821913, forward nfe 73000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 093, Runtime 0.253034, Loss 1.821835, forward nfe 73800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 094, Runtime 0.249719, Loss 1.821758, forward nfe 74600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 095, Runtime 0.252280, Loss 1.821681, forward nfe 75400, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 096, Runtime 0.256939, Loss 1.821606, forward nfe 76200, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 097, Runtime 0.671680, Loss 1.821531, forward nfe 77000, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 098, Runtime 0.244670, Loss 1.821456, forward nfe 77800, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "Epoch: 099, Runtime 0.253741, Loss 1.821383, forward nfe 78600, backward nfe 0, Train: 0.5500, Val: 0.4934, Test: 0.4685, Best time: 50.0000\n",
      "best val accuracy 0.493382 with test accuracy 0.468543 at epoch 9 and best time 50.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt['function'] = 'GCN'\n",
    "opt['time'] = 50\n",
    "_,_,_,init_state,final_state  = main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAATgElEQVR4nO3df2xd5X3H8c8n5qY4Ia3pYq3gODNaES1QSiQvdKLqLyhkjIa0KisMKjS0RfzBCmuLgGYq9AcbVSTWqa3URgOpVVN+bElTJEAQBFXLptDYifkRnNCUicYGFdPWgzSpEiff/XGvUye99rXvfa5P7uP3S7Lkc+6553wfXevjc5/znOc4IgQAyMe8ogsAAKRFsANAZgh2AMgMwQ4AmSHYASAzJxRx0MWLF0dPT08RhwaAltXf3/96RHTW2q6QYO/p6VFfX18RhwaAlmX75elsR1cMAGSGYAeAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADLTcLDb7rb9pO1B2zts35CiMABAfVLM7jgm6XMRsc32Ikn9tjdHxAsJ9g0AmKGGz9gj4tWI2Fb5/U1Jg5K6Gt0vAKA+SfvYbfdIWibp6SqvrbbdZ7tvZGQk5WEBABMkC3bbJ0naIOnGiHjj2NcjYl1E9EZEb2dnzQeAAADqlCTYbZdUDvX1EbExxT4BAPVJMSrGku6WNBgRdzVeEgCgESnO2M+X9GlJH7E9UPm5JMF+AQB1aHi4Y0Q8JckJagEAJMCdpwCQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBmCHQAyQ7ADQGYIdgDIDMEOAJkh2AEgMwQ7AGSGYAeAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZOaHoAoCpbNo+rLWP7tIro/t1ake7brr4DK1a1lV0WcBxzREx6wft7e2Nvr6+WT9uK0odbM0OypT737R9WLdufE77Dx46an1He0m3rzzrqP3yDwBzge3+iOituR3Bfvz6503Paf2WX2riJ1SaZ5104gka3XdwxgFWLSjbS23610+8J0kIptr/eEgPj+6vuW1He0mXvvcUbegfblq7gOPFrAa77RWS/l1Sm6T/iIg7p9qeYK9t0/Zh/dP9A6r16UwWYMeewX74XZ269+k9OlTl8+7qaNd/3/KRhms+/84nqobxxP1XO7OWdGTd29pL+t2BMR08lOLvUhpv7skLSrrtY2cR9GhpsxbsttskvSjpo5KGJG2VdGVEvDDZewj22iYLyWpOXlDS9i9edGR5si6MyVjS/9751/WUeZTTbnlo0n9EJy8o6bf7Dv7R+tI8S1aSIJ+Oq9+3VF9d9Z4jy3ThoJVMN9hTXDxdLml3RLxUOfB9ki6TNGmwo7ZXphnqkvTbfQe1afuwVi3r0qbtw/rsAwM6PIOc7FhQ0vl3PlF3uI2H41SHrBbqknRwJoUm8P0tv9RDz76q0X0H1bGgpL2/HztSw/Doft268TlJItzR0lIEe5ekPROWhySdd+xGtldLWi1JS5cuTXDY1lbrTPHUjvZpn7FL5a4MSbrpv56ZUahL5dAdD97h0f266T+fkVQ73MrfDJ7V/oOHZ3bAgo23tdo/m/0HD2nto7sIdrS0FF0xl0u6OCL+vrL8aUnLI+IfJ3vPXO+KmayrZJ6lw1Huk+75k3b9zy9+U7OPvVls6d/+5twj3wLG/wl1LCgpQhrdX/0MPAepuqaA1GazK2ZIUveE5SWSXkmw32wce3a+78BY1f7v8TPt4dH9Mzpbb4YI6cb7B/StJ3+uod/+/ki9k3Wp5OTUjvaiSwAakiLYt0o63fZpkoYlXSHpbxPsNwvHnp0XHdgz9fPXfld0CbNufKQO0KoaDvaIGLN9vaRHVR7ueE9E7Gi4skysfXTXtEenoHgd7SX619HykkwpEBEPS3o4xb5yM5PRLZgdC+e36XcHDsnSUdcw2kttun3lWUWVBSTDXDFNNtPRLWiuE9usHV9eIYkx7MgXwd5Em7YPa9+BsaLLwAQ777jkyO+rlnUR5MgSwd4kM737E81Vmmetvfy9RZcBzArmY2+SyS6attkFVDP3lNqsjvaSrPJ9AWsvfy9n55gzOGNvkskumlabhAtplOZJY4dFfznmPIK9SbhoOnu6CHLgKAR7k3z4XZ1/NJc60jp2pkYAZQR7E2zaPqwN/cOEepPwEA1galw8bQLuNm1Me2meTl7whwufV79vqbo62o8sE+rA1Dhjr9NUN7dwt+nMtNm68rxuulWARAj2OlSb2GviAxq4cDo9pTZr7ScZhgikRldMHap1tYw/oEFidsCpHDW2nFAHmoIz9jpM1tUyvn7Vsi7d/uCOrB9GUY9UD80GMDXO2Osw2YMYJq6/feVZ5Qc1Q1J5JAvfZIDZQbDX4aaLz1B7qe2odccG16plXTrpxLnzhairo11f/9S5R0avnLygdFS3CyNZgNkzd5InofGAqjXl6+gceIyc9Id/asyWCBwfCPY6TSfE3tZeyrKf/U8XzdcJbW3MYw4cpwj2Jhgf455bqHe0l3T7yrMIceA4R7Antmn7sD77wIAOZzSfAKNZgNbCxdPEvrDx2axCvdRmRrMALYZgT2zfwcNFl5DMwvlt3EQEtCC6YqBSm/Wpv+jWkztHuCAKZIBgT8yWWuEhSZYU4iEVQI4I9sSuOm+pvr/ll0WXUdXC+W3ad+AQZ+RA5hoKdttrJX1M0gFJv5D0dxExmqCulvXVVe/Rxv6h46qvnWGKwNzS6MXTzZLOjohzJL0o6dbGS2p9//KJc2b1eBNv55fK85tPXD9w20WEOjCHNHTGHhGPTVjcIumTjZWTh1XLunTj/QOzcixu5wdwrJTDHa+V9MhkL9pebbvPdt/IyEjCwx6fxs+aJzN+Nn31+5aq3jkgmVwLQDU1z9htPy7pHVVeWhMRP6pss0bSmKT1k+0nItZJWidJvb29LTBupDFXntdd9SLq+X/+dq3/h788srxqWZd6/+ztR00o9uF3dWpD//Ckz03lYc4AplIz2CPiwqlet32NpEslXRDRCgP9Zsf48zvvfXqPDkVM+VzPat0oE8O+Y0FJEdL/7T/IiBYANbmRLLa9QtJdkj4YEdPuX+nt7Y2+vr66jwsAc5Ht/ojorbVdo33s35S0SNJm2wO2v93g/gAADWp0VMw7UxUCAEiDScAAIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZKahScDqPqj9pqRds37g2bNY0utFF9FEObcv57ZJtK/VnRERi2ptVNTDrHdNZ4ayVmW7j/a1ppzbJtG+Vmd7WtPi0hUDAJkh2AEgM0UF+7qCjjtbaF/ryrltEu1rddNqXyEXTwEAzUNXDABkhmAHgMwUFuy2v2L72cqzUh+zfWpRtaRme63tnZX2/dB2R9E1pWT7cts7bB+2nc3QMtsrbO+yvdv2LUXXk5Lte2y/Zvv5omtpBtvdtp+0PVj527yh6JpSsX2i7Z/ZfqbSti/VfE9Rfey23xoRb1R+/4ykMyPiukKKScz2RZKeiIgx21+TpIi4ueCykrH9bkmHJX1H0ucjYlpja49nttskvSjpo5KGJG2VdGVEvFBoYYnY/oCkvZK+FxFnF11ParZPkXRKRGyzvUhSv6RVOXx+ti1pYUTstV2S9JSkGyJiy2TvKeyMfTzUKxZKyuYqbkQ8FhFjlcUtkpYUWU9qETEYEbndObxc0u6IeCkiDki6T9JlBdeUTET8RNJviq6jWSLi1YjYVvn9TUmDkrqKrSqNKNtbWSxVfqbMy0L72G3fYXuPpKskfbHIWproWkmPFF0EauqStGfC8pAyCYa5xnaPpGWSni64lGRst9kekPSapM0RMWXbmhrsth+3/XyVn8skKSLWRES3pPWSrm9mLanValtlmzWSxlRuX0uZTvsy4yrrsvkWOVfYPknSBkk3HtMr0NIi4lBEnKvyt//ltqfsTmvqXDERceE0N/2BpIck3dbEcpKq1Tbb10i6VNIF0YI3C8zgs8vFkKTuCctLJL1SUC2oQ6X/eYOk9RGxseh6miEiRm3/WNIKSZNeCC9yVMzpExZXStpZVC2p2V4h6WZJKyNiX9H1YFq2Sjrd9mm250u6QtKDBdeEaapcYLxb0mBE3FV0PSnZ7hwfWWe7XdKFqpGXRY6K2SDpDJVHV7ws6bqIGC6kmMRs75b0Fkm/rqzaksuIH0my/XFJ35DUKWlU0kBEXFxoUQnYvkTS1yW1SbonIu4otqJ0bN8r6UMqT2v7K0m3RcTdhRaVkO33S/qppOdUzhRJ+kJEPFxcVWnYPkfSd1X+u5wn6YGI+PKU72nBXgIAwBS48xQAMkOwA0BmCHYAyEwhj8ZbvHhx9PT0FHFoAGhZ/f39r0dEZ63tCgn2np4e9fW1/PQiADCrbL88ne3oigGAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADLTcLDb7rb9pO1B2zts35CiMABAfVJMAjYm6XMRsc32Ikn9tjdHxAsJ9g0AmKGGz9gj4tWI2Fb5/U1Jg5K6Gt0vAKA+SfvYbfdIWibp6SqvrbbdZ7tvZGQk5WEBABMkC3bbJ0naIOnGiHjj2NcjYl1E9EZEb2dnzXniAQB1ShLstksqh/r6iNiYYp8AgPqkGBVjSXdLGoyIuxovCQDQiBRn7OdL+rSkj9geqPxckmC/AIA6NDzcMSKekuQEtQAAEuDOUwDIDMEOAJkh2AEgMwQ7AGSGYAeAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBmCHQAyQ7ADQGaSBLvtFbZ32d5t+5YU+wQA1KfhYLfdJulbkv5K0pmSrrR9ZqP7BQDUJ8UZ+3JJuyPipYg4IOk+SZcl2C8AoA4pgr1L0p4Jy0OVdUexvdp2n+2+kZGRBIcFAFSTIthdZV380YqIdRHRGxG9nZ2dCQ4LAKgmRbAPSeqesLxE0isJ9gsAqEOKYN8q6XTbp9meL+kKSQ8m2C8AoA4nNLqDiBizfb2kRyW1SbonInY0XBkAoC4NB7skRcTDkh5OsS8AQGO48xQAMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBmCHQAyQ7ADQGYIdgDIDMEOAJkh2AEgMwQ7AGSGYAeAzBDsAJCZhoLd9lrbO20/a/uHtjsS1QUAqFOjZ+ybJZ0dEedIelHSrY2XBABoREPBHhGPRcRYZXGLpCWNlwQAaETKPvZrJT0y2Yu2V9vus903MjKS8LAAgIlOqLWB7cclvaPKS2si4keVbdZIGpO0frL9RMQ6Seskqbe3N+qqFgBQU81gj4gLp3rd9jWSLpV0QUQQ2ABQsJrBPhXbKyTdLOmDEbEvTUkAgEY02sf+TUmLJG22PWD72wlqAgA0oKEz9oh4Z6pCAABpcOcpAGSGYAeAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGRcx067tNyXtmvUDz57Fkl4vuogmyrl9ObdNon2t7oyIWFRro4YmAWvArojoLejYTWe7j/a1ppzbJtG+Vme7bzrb0RUDAJkh2AEgM0UF+7qCjjtbaF/ryrltEu1rddNqXyEXTwEAzUNXDABkhmAHgMwUFuy2v2L72cpDsB+zfWpRtaRme63tnZX2/dB2R9E1pWT7cts7bB+2nc3QMtsrbO+yvdv2LUXXk5Lte2y/Zvv5omtpBtvdtp+0PVj527yh6JpSsX2i7Z/ZfqbSti/VfE9Rfey23xoRb1R+/4ykMyPiukKKScz2RZKeiIgx21+TpIi4ueCykrH9bkmHJX1H0ucjYlpja49nttskvSjpo5KGJG2VdGVEvFBoYYnY/oCkvZK+FxFnF11ParZPkXRKRGyzvUhSv6RVOXx+ti1pYUTstV2S9JSkGyJiy2TvKeyMfTzUKxZKyuYqbkQ8FhFjlcUtkpYUWU9qETEYEbndObxc0u6IeCkiDki6T9JlBdeUTET8RNJviq6jWSLi1YjYVvn9TUmDkrqKrSqNKNtbWSxVfqbMy0L72G3fYXuPpKskfbHIWproWkmPFF0EauqStGfC8pAyCYa5xnaPpGWSni64lGRst9kekPSapM0RMWXbmhrsth+3/XyVn8skKSLWRES3pPWSrm9mLanValtlmzWSxlRuX0uZTvsy4yrrsvkWOVfYPknSBkk3HtMr0NIi4lBEnKvyt//ltqfsTmvqXDERceE0N/2BpIck3dbEcpKq1Tbb10i6VNIF0YI3C8zgs8vFkKTuCctLJL1SUC2oQ6X/eYOk9RGxseh6miEiRm3/WNIKSZNeCC9yVMzpExZXStpZVC2p2V4h6WZJKyNiX9H1YFq2Sjrd9mm250u6QtKDBdeEaapcYLxb0mBE3FV0PSnZ7hwfWWe7XdKFqpGXRY6K2SDpDJVHV7ws6bqIGC6kmMRs75b0Fkm/rqzaksuIH0my/XFJ35DUKWlU0kBEXFxoUQnYvkTS1yW1SbonIu4otqJ0bN8r6UMqT2v7K0m3RcTdhRaVkO33S/qppOdUzhRJ+kJEPFxcVWnYPkfSd1X+u5wn6YGI+PKU72nBXgIAwBS48xQAMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMz8P9w2tL3nyTI+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_state(init_state,final_state)\n",
    "print(final_state.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "using ACMP\n",
      "using ACMP\n",
      "using Cora dataset\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'barrier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/kai/ACMP/src/ACMP_illustration.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m opt[\u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdopri5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m opt[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m18\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22494e535f41313030227d/home/kai/ACMP/src/ACMP_illustration.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m _,_,test_acc,init_state,final_state \u001b[39m=\u001b[39m main(opt)\n",
      "File \u001b[0;32m~/ACMP/src/run_GNN.py:262\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cmd_opt)\u001b[0m\n\u001b[1;32m    259\u001b[0m   ei \u001b[39m=\u001b[39m apply_KNN(data, pos_encoding, model, opt)\n\u001b[1;32m    260\u001b[0m   model\u001b[39m.\u001b[39modeblock\u001b[39m.\u001b[39modefunc\u001b[39m.\u001b[39medge_index \u001b[39m=\u001b[39m ei\n\u001b[0;32m--> 262\u001b[0m loss \u001b[39m=\u001b[39m train(model, optimizer, data, pos_encoding)\n\u001b[1;32m    263\u001b[0m tmp_train_acc, tmp_val_acc, tmp_test_acc \u001b[39m=\u001b[39m this_test(model, data, pos_encoding, opt)\n\u001b[1;32m    264\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ACMP/src/run_GNN.py:76\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data, pos_encoding)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m   train_pred_idx \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtrain_mask\n\u001b[0;32m---> 76\u001b[0m out \u001b[39m=\u001b[39m model(feat, pos_encoding)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mogbn-arxiv\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     79\u001b[0m   lf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnll_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ACMP/src/GNN_early.py:85\u001b[0m, in \u001b[0;36mGNNEarly.forward\u001b[0;34m(self, x, pos_encoding)\u001b[0m\n\u001b[1;32m     83\u001b[0m   z, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_states  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39modeblock(x)\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m   z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modeblock(x)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39maugment\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     88\u001b[0m   z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(z, x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ACMP/src/block_constant.py:57\u001b[0m, in \u001b[0;36mConstantODEblock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m   state_dt \u001b[39m=\u001b[39m integrator(\n\u001b[1;32m     47\u001b[0m     func, state, t,\n\u001b[1;32m     48\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     adjoint_atol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matol_adjoint,\n\u001b[1;32m     55\u001b[0m     adjoint_rtol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrtol_adjoint)\n\u001b[1;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m   state_dt \u001b[39m=\u001b[39m integrator(\n\u001b[1;32m     58\u001b[0m     func, state, t,\n\u001b[1;32m     59\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mmethod\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     60\u001b[0m     options\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(step_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mstep_size\u001b[39;49m\u001b[39m'\u001b[39;49m], max_iters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mmax_iters\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     61\u001b[0m     atol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matol,\n\u001b[1;32m     62\u001b[0m     rtol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrtol)\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnreg \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m   z \u001b[39m=\u001b[39m state_dt[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[39m=\u001b[39m SOLVERS[method](func\u001b[39m=\u001b[39mfunc, y0\u001b[39m=\u001b[39my0, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mintegrate(t)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39mintegrate_until_event(t[\u001b[39m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py:28\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m solution[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my0\n\u001b[1;32m     27\u001b[0m t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(t)):\n\u001b[1;32m     30\u001b[0m     solution[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_advance(t[i])\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py:161\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_before_integrate\u001b[39m(\u001b[39mself\u001b[39m, t):\n\u001b[1;32m    160\u001b[0m     t0 \u001b[39m=\u001b[39m t[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m     f0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(t[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my0)\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_step \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         first_step \u001b[39m=\u001b[39m _select_initial_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, t[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrtol, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matol,\n\u001b[1;32m    164\u001b[0m                                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm, f0\u001b[39m=\u001b[39mf0)\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_func(t, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/ACMP/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ACMP/src/function_laplacian_diffusion_ACMP.py:57\u001b[0m, in \u001b[0;36mLaplacianODEFunc.forward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m   f \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mdelta\u001b[39m*\u001b[39m(x\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mbarrier\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m diffusion\u001b[39m@alpha\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m   f \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mdelta\u001b[39m*\u001b[39m(x\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mbarrier\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m alpha\u001b[39m*\u001b[39mdiffusion\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39madd_source\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     60\u001b[0m   f \u001b[39m=\u001b[39m f \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_train \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx0\n",
      "\u001b[0;31mKeyError\u001b[0m: 'barrier'"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'ACMP'\n",
    "opt['method'] = 'dopri5'\n",
    "opt['time'] = 18\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.444619, Loss 1.948222, forward nfe 116, backward nfe 0, Train: 0.5286, Val: 0.5500, Test: 0.5199, Best time: 14.5957\n",
      "Epoch: 002, Runtime 0.538288, Loss 1.789718, forward nfe 966, backward nfe 0, Train: 0.5286, Val: 0.5500, Test: 0.5199, Best time: 50.0000\n",
      "Epoch: 003, Runtime 0.618318, Loss 1.565958, forward nfe 1882, backward nfe 0, Train: 0.8143, Val: 0.6074, Test: 0.6101, Best time: 16.1191\n",
      "Epoch: 004, Runtime 0.634353, Loss 1.364492, forward nfe 2828, backward nfe 0, Train: 0.8286, Val: 0.7831, Test: 0.7823, Best time: 77.2215\n",
      "Epoch: 005, Runtime 0.639800, Loss 1.119564, forward nfe 3798, backward nfe 0, Train: 0.8357, Val: 0.7875, Test: 0.7947, Best time: 75.7355\n",
      "Epoch: 006, Runtime 0.649337, Loss 0.971101, forward nfe 4786, backward nfe 0, Train: 0.8357, Val: 0.7875, Test: 0.7947, Best time: 50.0000\n",
      "Epoch: 007, Runtime 0.670543, Loss 0.808915, forward nfe 5792, backward nfe 0, Train: 0.8357, Val: 0.7875, Test: 0.7947, Best time: 50.0000\n",
      "Epoch: 008, Runtime 0.688181, Loss 0.701375, forward nfe 6810, backward nfe 0, Train: 0.8357, Val: 0.7875, Test: 0.7947, Best time: 50.0000\n",
      "Epoch: 009, Runtime 0.699348, Loss 0.635218, forward nfe 7834, backward nfe 0, Train: 0.8357, Val: 0.7875, Test: 0.7947, Best time: 50.0000\n",
      "Epoch: 010, Runtime 0.691961, Loss 0.564828, forward nfe 8858, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 011, Runtime 0.686534, Loss 0.510296, forward nfe 9876, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 012, Runtime 0.711496, Loss 0.485377, forward nfe 10918, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 013, Runtime 0.719760, Loss 0.426959, forward nfe 11960, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 014, Runtime 0.688674, Loss 0.435321, forward nfe 12996, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 015, Runtime 0.740420, Loss 0.430002, forward nfe 14038, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 016, Runtime 0.720943, Loss 0.364405, forward nfe 15086, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 017, Runtime 0.708405, Loss 0.361792, forward nfe 16128, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 018, Runtime 0.699301, Loss 0.362125, forward nfe 17170, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 019, Runtime 0.695564, Loss 0.339803, forward nfe 18212, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 020, Runtime 0.695886, Loss 0.341189, forward nfe 19254, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 021, Runtime 0.690156, Loss 0.324261, forward nfe 20302, backward nfe 0, Train: 0.8929, Val: 0.8059, Test: 0.8220, Best time: 50.0000\n",
      "Epoch: 022, Runtime 0.700531, Loss 0.353858, forward nfe 21350, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 023, Runtime 0.699945, Loss 0.296815, forward nfe 22386, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 024, Runtime 0.808730, Loss 0.333927, forward nfe 23416, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 025, Runtime 0.676104, Loss 0.307035, forward nfe 24446, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 026, Runtime 0.689685, Loss 0.293114, forward nfe 25488, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 027, Runtime 0.676071, Loss 0.268319, forward nfe 26518, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 028, Runtime 0.680225, Loss 0.270312, forward nfe 27536, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 029, Runtime 0.673334, Loss 0.282777, forward nfe 28548, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 030, Runtime 0.672056, Loss 0.253227, forward nfe 29578, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 031, Runtime 0.668659, Loss 0.254286, forward nfe 30590, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 032, Runtime 0.688196, Loss 0.268319, forward nfe 31632, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 033, Runtime 0.698176, Loss 0.296300, forward nfe 32674, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 034, Runtime 0.720600, Loss 0.241421, forward nfe 33734, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 035, Runtime 0.715385, Loss 0.266069, forward nfe 34788, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 036, Runtime 0.809987, Loss 0.229355, forward nfe 35824, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 037, Runtime 0.690451, Loss 0.234836, forward nfe 36860, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 038, Runtime 0.675452, Loss 0.260785, forward nfe 37878, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 039, Runtime 0.689995, Loss 0.212644, forward nfe 38920, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 040, Runtime 0.706826, Loss 0.262409, forward nfe 39968, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 041, Runtime 0.698678, Loss 0.269236, forward nfe 41022, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 042, Runtime 0.711806, Loss 0.225208, forward nfe 42082, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 043, Runtime 0.693229, Loss 0.247320, forward nfe 43148, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 044, Runtime 0.700426, Loss 0.221955, forward nfe 44196, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 045, Runtime 0.714344, Loss 0.252587, forward nfe 45256, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 046, Runtime 0.740723, Loss 0.237954, forward nfe 46340, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 047, Runtime 0.735984, Loss 0.205978, forward nfe 47430, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 048, Runtime 0.739188, Loss 0.249421, forward nfe 48526, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 049, Runtime 0.734936, Loss 0.248242, forward nfe 49622, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 050, Runtime 0.734242, Loss 0.206149, forward nfe 50730, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 051, Runtime 0.742016, Loss 0.204675, forward nfe 51832, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 052, Runtime 0.730347, Loss 0.231322, forward nfe 52928, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 053, Runtime 0.716996, Loss 0.237963, forward nfe 54018, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 054, Runtime 0.715539, Loss 0.269191, forward nfe 55108, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 055, Runtime 0.731784, Loss 0.195877, forward nfe 56210, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 056, Runtime 0.746523, Loss 0.232999, forward nfe 57324, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 057, Runtime 0.760743, Loss 0.189913, forward nfe 58450, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 058, Runtime 0.766647, Loss 0.230905, forward nfe 59576, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 059, Runtime 0.761269, Loss 0.216726, forward nfe 60702, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 060, Runtime 0.750129, Loss 0.189858, forward nfe 61840, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 061, Runtime 0.763999, Loss 0.244299, forward nfe 62960, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 062, Runtime 0.760741, Loss 0.187836, forward nfe 64086, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 063, Runtime 0.769293, Loss 0.198396, forward nfe 65212, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 064, Runtime 0.781955, Loss 0.197194, forward nfe 66332, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 065, Runtime 0.916026, Loss 0.193784, forward nfe 67446, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 066, Runtime 0.746020, Loss 0.208313, forward nfe 68560, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 067, Runtime 0.735380, Loss 0.193248, forward nfe 69674, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 068, Runtime 0.750238, Loss 0.194107, forward nfe 70806, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 069, Runtime 0.786107, Loss 0.180672, forward nfe 71974, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 070, Runtime 0.793969, Loss 0.194006, forward nfe 73148, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 071, Runtime 0.809942, Loss 0.196932, forward nfe 74334, backward nfe 0, Train: 0.9500, Val: 0.8096, Test: 0.7997, Best time: 50.0000\n",
      "Epoch: 072, Runtime 0.808973, Loss 0.175461, forward nfe 75532, backward nfe 0, Train: 0.9714, Val: 0.8110, Test: 0.8063, Best time: 45.1201\n",
      "Epoch: 073, Runtime 0.832467, Loss 0.205741, forward nfe 76724, backward nfe 0, Train: 0.9714, Val: 0.8110, Test: 0.8063, Best time: 50.0000\n",
      "Epoch: 074, Runtime 0.828600, Loss 0.178484, forward nfe 77922, backward nfe 0, Train: 0.9714, Val: 0.8110, Test: 0.8063, Best time: 50.0000\n",
      "Epoch: 075, Runtime 0.798971, Loss 0.172589, forward nfe 79108, backward nfe 0, Train: 0.9714, Val: 0.8110, Test: 0.8063, Best time: 50.0000\n",
      "Epoch: 076, Runtime 0.797131, Loss 0.156534, forward nfe 80288, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 077, Runtime 0.801237, Loss 0.144709, forward nfe 81462, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 078, Runtime 0.775540, Loss 0.185864, forward nfe 82630, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 079, Runtime 0.796828, Loss 0.199369, forward nfe 83798, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 080, Runtime 0.816071, Loss 0.174165, forward nfe 85002, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 081, Runtime 0.887794, Loss 0.197810, forward nfe 86212, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 082, Runtime 0.844483, Loss 0.222179, forward nfe 87446, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 083, Runtime 0.841733, Loss 0.158218, forward nfe 88686, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 084, Runtime 0.856783, Loss 0.168382, forward nfe 89926, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 085, Runtime 0.864393, Loss 0.174607, forward nfe 91178, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 086, Runtime 0.843537, Loss 0.185232, forward nfe 92430, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 087, Runtime 0.847918, Loss 0.234543, forward nfe 93676, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 088, Runtime 0.840652, Loss 0.170988, forward nfe 94922, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 089, Runtime 0.853933, Loss 0.184727, forward nfe 96168, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 090, Runtime 0.860093, Loss 0.191727, forward nfe 97408, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 091, Runtime 0.850674, Loss 0.180533, forward nfe 98660, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 092, Runtime 0.850819, Loss 0.159299, forward nfe 99918, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 093, Runtime 0.875239, Loss 0.165432, forward nfe 101176, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 094, Runtime 0.946795, Loss 0.163248, forward nfe 102482, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 095, Runtime 0.955443, Loss 0.182249, forward nfe 103800, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 096, Runtime 1.023087, Loss 0.141497, forward nfe 105130, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 097, Runtime 0.930039, Loss 0.161649, forward nfe 106484, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 098, Runtime 0.962280, Loss 0.169747, forward nfe 107844, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "Epoch: 099, Runtime 0.965150, Loss 0.184937, forward nfe 109222, backward nfe 0, Train: 0.9786, Val: 0.8162, Test: 0.8204, Best time: 50.0000\n",
      "best val accuracy 0.816176 with test accuracy 0.820364 at epoch 76 and best time 50.000000\n"
     ]
    }
   ],
   "source": [
    "opt['function'] = 'ACMP'\n",
    "opt['method'] = 'dopri5'\n",
    "opt['time'] = 50\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterophilic Dataset\n",
    "\n",
    "Texas homophily level: 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "using GCN\n",
      "using GCN\n",
      "Epoch: 001, Runtime 0.193205, Loss 7.090044, forward nfe 86, backward nfe 0, Train: 0.2299, Val: 0.0847, Test: 0.1351, Best time: 5.0000\n",
      "Epoch: 002, Runtime 0.201734, Loss 176.495010, forward nfe 468, backward nfe 0, Train: 0.2299, Val: 0.3390, Test: 0.1622, Best time: 0.0416\n",
      "Epoch: 003, Runtime 0.196142, Loss 85.560257, forward nfe 838, backward nfe 0, Train: 0.2299, Val: 0.3390, Test: 0.1622, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.200840, Loss 59.713978, forward nfe 1214, backward nfe 0, Train: 0.2299, Val: 0.3390, Test: 0.1622, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.199876, Loss 16.672255, forward nfe 1596, backward nfe 0, Train: 0.7471, Val: 0.4915, Test: 0.5135, Best time: 0.0391\n",
      "Epoch: 006, Runtime 0.199842, Loss 2.553453, forward nfe 1984, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 1.1197\n",
      "Epoch: 007, Runtime 0.204156, Loss 2.505577, forward nfe 2372, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.205291, Loss 3.314673, forward nfe 2760, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.208081, Loss 3.101156, forward nfe 3154, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.202709, Loss 2.578519, forward nfe 3548, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.205717, Loss 2.123664, forward nfe 3942, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.209454, Loss 1.853346, forward nfe 4336, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.215742, Loss 1.414043, forward nfe 4730, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.204473, Loss 1.229189, forward nfe 5124, backward nfe 0, Train: 0.7586, Val: 0.5424, Test: 0.6757, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.208965, Loss 1.064236, forward nfe 5512, backward nfe 0, Train: 0.7816, Val: 0.5593, Test: 0.7027, Best time: 1.0756\n",
      "Epoch: 016, Runtime 0.207672, Loss 0.916066, forward nfe 5906, backward nfe 0, Train: 0.7701, Val: 0.5763, Test: 0.7027, Best time: 0.3008\n",
      "Epoch: 017, Runtime 0.203332, Loss 0.985260, forward nfe 6300, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 1.0763\n",
      "Epoch: 018, Runtime 0.205895, Loss 0.981452, forward nfe 6694, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.202299, Loss 0.898912, forward nfe 7088, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.199251, Loss 1.152092, forward nfe 7476, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.203898, Loss 1.067541, forward nfe 7864, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.202433, Loss 0.855999, forward nfe 8252, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.200040, Loss 0.880835, forward nfe 8640, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.202161, Loss 0.789968, forward nfe 9028, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.199069, Loss 0.809859, forward nfe 9416, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.212250, Loss 0.820682, forward nfe 9804, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.203245, Loss 0.832791, forward nfe 10192, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.203838, Loss 0.745806, forward nfe 10580, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.204888, Loss 0.734533, forward nfe 10968, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.203850, Loss 0.738193, forward nfe 11356, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.202660, Loss 0.709484, forward nfe 11744, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.203211, Loss 0.663237, forward nfe 12132, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.203023, Loss 0.727421, forward nfe 12520, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.205580, Loss 0.637638, forward nfe 12908, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.203287, Loss 0.593353, forward nfe 13296, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.202799, Loss 0.606662, forward nfe 13684, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.201157, Loss 0.614850, forward nfe 14072, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.207444, Loss 0.565215, forward nfe 14460, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.196603, Loss 0.538505, forward nfe 14848, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.212425, Loss 0.561410, forward nfe 15236, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.206053, Loss 0.725235, forward nfe 15624, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.204149, Loss 0.547987, forward nfe 16012, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.204112, Loss 0.527960, forward nfe 16400, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.204871, Loss 0.558740, forward nfe 16788, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.205429, Loss 0.527289, forward nfe 17176, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.204866, Loss 0.465621, forward nfe 17564, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.204759, Loss 0.469099, forward nfe 17952, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.207052, Loss 0.482225, forward nfe 18340, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.204559, Loss 0.498132, forward nfe 18728, backward nfe 0, Train: 0.7816, Val: 0.5932, Test: 0.7027, Best time: 5.0000\n",
      "best val accuracy 0.593220 with test accuracy 0.702703 at epoch 17 and best time 5.000000\n"
     ]
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "\n",
    "opt['function'] = 'GCN'\n",
    "\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.0\n",
    "\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.088917, Loss 1.568276, forward nfe 38, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.7838, Best time: 1.3983\n",
      "Epoch: 002, Runtime 0.095202, Loss 1.423147, forward nfe 156, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 1.6230\n",
      "Epoch: 003, Runtime 0.101707, Loss 1.222587, forward nfe 286, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.111951, Loss 0.982933, forward nfe 428, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.122745, Loss 0.744253, forward nfe 582, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.126677, Loss 0.542812, forward nfe 748, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.139416, Loss 0.444855, forward nfe 932, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.141462, Loss 0.381892, forward nfe 1116, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.139609, Loss 0.303005, forward nfe 1300, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.131405, Loss 0.264928, forward nfe 1478, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.131768, Loss 0.233964, forward nfe 1656, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.137245, Loss 0.181998, forward nfe 1828, backward nfe 0, Train: 0.8391, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.124686, Loss 0.176681, forward nfe 1994, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 3.6095\n",
      "Epoch: 014, Runtime 0.134107, Loss 0.143474, forward nfe 2166, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.131107, Loss 0.185615, forward nfe 2344, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.128081, Loss 0.152337, forward nfe 2522, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.141572, Loss 0.134529, forward nfe 2700, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.137007, Loss 0.122487, forward nfe 2884, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.132708, Loss 0.159575, forward nfe 3056, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.128446, Loss 0.118923, forward nfe 3222, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.133562, Loss 0.150866, forward nfe 3388, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.130075, Loss 0.101523, forward nfe 3554, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.136064, Loss 0.097745, forward nfe 3726, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.119654, Loss 0.095554, forward nfe 3886, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.121337, Loss 0.116682, forward nfe 4046, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.115429, Loss 0.095257, forward nfe 4200, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.112877, Loss 0.110801, forward nfe 4342, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.116373, Loss 0.106735, forward nfe 4490, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.127364, Loss 0.063440, forward nfe 4638, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.118349, Loss 0.068962, forward nfe 4786, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.104335, Loss 0.083718, forward nfe 4922, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.109258, Loss 0.060772, forward nfe 5058, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.103619, Loss 0.090268, forward nfe 5194, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.106760, Loss 0.068617, forward nfe 5330, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.090706, Loss 0.080199, forward nfe 5454, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.106496, Loss 0.050928, forward nfe 5584, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.096564, Loss 0.059778, forward nfe 5708, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.094017, Loss 0.079667, forward nfe 5832, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.096838, Loss 0.069617, forward nfe 5956, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.095708, Loss 0.079034, forward nfe 6080, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.095022, Loss 0.079044, forward nfe 6204, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.097919, Loss 0.064980, forward nfe 6328, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.093521, Loss 0.106442, forward nfe 6452, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.088916, Loss 0.073848, forward nfe 6576, backward nfe 0, Train: 0.9770, Val: 0.8305, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.088996, Loss 0.065027, forward nfe 6688, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.086090, Loss 0.068910, forward nfe 6800, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.086423, Loss 0.075779, forward nfe 6912, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.095834, Loss 0.051161, forward nfe 7024, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.098339, Loss 0.068937, forward nfe 7136, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 050, Runtime 0.094816, Loss 0.085267, forward nfe 7260, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 051, Runtime 0.096977, Loss 0.079675, forward nfe 7384, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 052, Runtime 0.101410, Loss 0.077548, forward nfe 7514, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 053, Runtime 0.102072, Loss 0.076254, forward nfe 7644, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 054, Runtime 0.101221, Loss 0.074428, forward nfe 7774, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 055, Runtime 0.095082, Loss 0.082226, forward nfe 7898, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 056, Runtime 0.096370, Loss 0.065977, forward nfe 8022, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 057, Runtime 0.093505, Loss 0.083655, forward nfe 8146, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 058, Runtime 0.102292, Loss 0.054084, forward nfe 8270, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 059, Runtime 0.097536, Loss 0.045990, forward nfe 8394, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 060, Runtime 0.097027, Loss 0.057728, forward nfe 8518, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 061, Runtime 0.094784, Loss 0.064052, forward nfe 8642, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 062, Runtime 0.096079, Loss 0.067537, forward nfe 8766, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 063, Runtime 0.087282, Loss 0.042833, forward nfe 8890, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 064, Runtime 0.087020, Loss 0.057870, forward nfe 9002, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 065, Runtime 0.090981, Loss 0.030958, forward nfe 9114, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 066, Runtime 0.089175, Loss 0.079687, forward nfe 9226, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 067, Runtime 0.089052, Loss 0.053400, forward nfe 9338, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 068, Runtime 0.091276, Loss 0.043892, forward nfe 9450, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 069, Runtime 0.085970, Loss 0.049631, forward nfe 9562, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 070, Runtime 0.093533, Loss 0.051949, forward nfe 9674, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 071, Runtime 0.095267, Loss 0.066742, forward nfe 9798, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 072, Runtime 0.088783, Loss 0.033029, forward nfe 9922, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 073, Runtime 0.096086, Loss 0.046459, forward nfe 10034, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 074, Runtime 0.098531, Loss 0.056213, forward nfe 10158, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 075, Runtime 0.097664, Loss 0.041129, forward nfe 10288, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 076, Runtime 0.096091, Loss 0.055546, forward nfe 10418, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 077, Runtime 0.100408, Loss 0.038995, forward nfe 10548, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 078, Runtime 0.100385, Loss 0.050265, forward nfe 10678, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 079, Runtime 0.100636, Loss 0.037133, forward nfe 10814, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 080, Runtime 0.100672, Loss 0.038752, forward nfe 10944, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 081, Runtime 0.093951, Loss 0.040323, forward nfe 11074, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 082, Runtime 0.102729, Loss 0.035560, forward nfe 11198, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 083, Runtime 0.093572, Loss 0.049734, forward nfe 11316, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 084, Runtime 0.092555, Loss 0.050861, forward nfe 11434, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 085, Runtime 0.095986, Loss 0.040426, forward nfe 11552, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 086, Runtime 0.101882, Loss 0.061576, forward nfe 11682, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 087, Runtime 0.098326, Loss 0.036453, forward nfe 11806, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 088, Runtime 0.102316, Loss 0.052805, forward nfe 11930, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 089, Runtime 0.107174, Loss 0.047541, forward nfe 12066, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 090, Runtime 0.107438, Loss 0.057155, forward nfe 12208, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 091, Runtime 0.101225, Loss 0.039873, forward nfe 12344, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 092, Runtime 0.096138, Loss 0.035243, forward nfe 12480, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 093, Runtime 0.097545, Loss 0.039551, forward nfe 12604, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 094, Runtime 0.097010, Loss 0.052470, forward nfe 12734, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 095, Runtime 0.097224, Loss 0.033871, forward nfe 12864, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 096, Runtime 0.092112, Loss 0.046404, forward nfe 12994, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 097, Runtime 0.093068, Loss 0.044675, forward nfe 13118, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 098, Runtime 0.099007, Loss 0.032207, forward nfe 13242, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 099, Runtime 0.110491, Loss 0.040654, forward nfe 13378, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "best val accuracy 0.881356 with test accuracy 0.891892 at epoch 46 and best time 5.000000\n"
     ]
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "# opt['not_lcc'] = 'False'\n",
    "opt['function'] = 'ACMP'\n",
    "\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.5\n",
    "\n",
    "_,_,test_acc,init_state,final_state = main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$ ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.128442, Loss 1.571433, forward nfe 38, backward nfe 0, Train: 0.5402, Val: 0.5254, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 002, Runtime 0.169291, Loss 1.480924, forward nfe 288, backward nfe 0, Train: 0.5402, Val: 0.5254, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.169866, Loss 1.259138, forward nfe 532, backward nfe 0, Train: 0.7701, Val: 0.5763, Test: 0.5946, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.192499, Loss 1.001834, forward nfe 794, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 6.3827\n",
      "Epoch: 005, Runtime 0.192070, Loss 0.876789, forward nfe 1068, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.208755, Loss 0.762516, forward nfe 1366, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.214738, Loss 0.666319, forward nfe 1664, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.217986, Loss 0.523098, forward nfe 1968, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.219746, Loss 0.399370, forward nfe 2278, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.233668, Loss 0.355602, forward nfe 2588, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.226609, Loss 0.327991, forward nfe 2928, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.240737, Loss 0.253053, forward nfe 3256, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.239393, Loss 0.209148, forward nfe 3602, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.258056, Loss 0.154004, forward nfe 3948, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.270090, Loss 0.167695, forward nfe 4342, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.277051, Loss 0.200510, forward nfe 4736, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.273919, Loss 0.152794, forward nfe 5142, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.290800, Loss 0.113644, forward nfe 5560, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.287173, Loss 0.129342, forward nfe 5990, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.296277, Loss 0.105425, forward nfe 6432, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.288623, Loss 0.132496, forward nfe 6868, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.308045, Loss 0.121828, forward nfe 7316, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.281678, Loss 0.077769, forward nfe 7764, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.283418, Loss 0.091862, forward nfe 8200, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.289827, Loss 0.083029, forward nfe 8654, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.283958, Loss 0.120023, forward nfe 9102, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.285479, Loss 0.088570, forward nfe 9538, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.294491, Loss 0.092667, forward nfe 9986, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7297, Best time: 0.3137\n",
      "Epoch: 029, Runtime 0.294410, Loss 0.118507, forward nfe 10440, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.294039, Loss 0.081241, forward nfe 10888, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7297, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.299474, Loss 0.100491, forward nfe 11336, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 0.0586\n",
      "Epoch: 032, Runtime 0.291622, Loss 0.096930, forward nfe 11784, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.297374, Loss 0.085260, forward nfe 12226, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.292389, Loss 0.085468, forward nfe 12674, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.300680, Loss 0.111250, forward nfe 13122, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.300293, Loss 0.127801, forward nfe 13570, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.287895, Loss 0.139421, forward nfe 14018, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.286925, Loss 0.138337, forward nfe 14454, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.291118, Loss 0.104601, forward nfe 14896, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.282989, Loss 0.099278, forward nfe 15350, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.291546, Loss 0.080051, forward nfe 15792, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.285514, Loss 0.113637, forward nfe 16240, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.297230, Loss 0.094684, forward nfe 16688, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.307551, Loss 0.064974, forward nfe 17148, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.292080, Loss 0.099229, forward nfe 17608, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.296479, Loss 0.064520, forward nfe 18068, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.298737, Loss 0.091507, forward nfe 18528, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.308321, Loss 0.070997, forward nfe 18988, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.291923, Loss 0.100688, forward nfe 19448, backward nfe 0, Train: 0.9885, Val: 0.7458, Test: 0.7838, Best time: 5.0000\n",
      "best val accuracy 0.745763 with test accuracy 0.783784 at epoch 31 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.105841, Loss 1.650028, forward nfe 32, backward nfe 0, Train: 0.5287, Val: 0.5254, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 002, Runtime 0.143202, Loss 1.342822, forward nfe 222, backward nfe 0, Train: 0.5287, Val: 0.5254, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.153514, Loss 1.039662, forward nfe 442, backward nfe 0, Train: 0.5287, Val: 0.5254, Test: 0.6486, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.161564, Loss 0.840216, forward nfe 674, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 0.0567\n",
      "Epoch: 005, Runtime 0.175108, Loss 0.643807, forward nfe 918, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.186583, Loss 0.486125, forward nfe 1174, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.197613, Loss 0.394812, forward nfe 1436, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.214260, Loss 0.315501, forward nfe 1710, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.223032, Loss 0.259874, forward nfe 2002, backward nfe 0, Train: 0.9425, Val: 0.6441, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.196240, Loss 0.242098, forward nfe 2312, backward nfe 0, Train: 0.9655, Val: 0.7119, Test: 0.6757, Best time: 0.0572\n",
      "Epoch: 011, Runtime 0.215259, Loss 0.198668, forward nfe 2616, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 0.5004\n",
      "Epoch: 012, Runtime 0.222268, Loss 0.147245, forward nfe 2944, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.219145, Loss 0.168179, forward nfe 3278, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.221144, Loss 0.135790, forward nfe 3612, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.219543, Loss 0.114771, forward nfe 3952, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.233647, Loss 0.138445, forward nfe 4292, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.235121, Loss 0.109429, forward nfe 4638, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.238409, Loss 0.103710, forward nfe 4990, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.237395, Loss 0.128779, forward nfe 5342, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.234602, Loss 0.066466, forward nfe 5694, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.235827, Loss 0.066013, forward nfe 6046, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.231116, Loss 0.081309, forward nfe 6392, backward nfe 0, Train: 0.9885, Val: 0.7288, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.227376, Loss 0.118196, forward nfe 6744, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 0.0627\n",
      "Epoch: 024, Runtime 0.223244, Loss 0.106676, forward nfe 7090, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.221029, Loss 0.102841, forward nfe 7436, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.227782, Loss 0.120338, forward nfe 7776, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.228012, Loss 0.089883, forward nfe 8122, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.219921, Loss 0.081758, forward nfe 8462, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.227633, Loss 0.115408, forward nfe 8802, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.224666, Loss 0.094320, forward nfe 9142, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.232613, Loss 0.087596, forward nfe 9488, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.234812, Loss 0.085412, forward nfe 9834, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.224199, Loss 0.102776, forward nfe 10180, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.216316, Loss 0.111911, forward nfe 10514, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.222976, Loss 0.121897, forward nfe 10866, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.223757, Loss 0.108223, forward nfe 11218, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.221116, Loss 0.109519, forward nfe 11570, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.218802, Loss 0.099102, forward nfe 11910, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.229981, Loss 0.087069, forward nfe 12256, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.223222, Loss 0.100749, forward nfe 12590, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.215672, Loss 0.116276, forward nfe 12930, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.220179, Loss 0.084840, forward nfe 13276, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.218241, Loss 0.084134, forward nfe 13616, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.221947, Loss 0.117265, forward nfe 13956, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.205615, Loss 0.098123, forward nfe 14278, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.205698, Loss 0.081102, forward nfe 14600, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.206122, Loss 0.056517, forward nfe 14922, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.214161, Loss 0.055861, forward nfe 15244, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.210164, Loss 0.042196, forward nfe 15566, backward nfe 0, Train: 1.0000, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "best val accuracy 0.762712 with test accuracy 0.837838 at epoch 23 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.097546, Loss 1.633226, forward nfe 32, backward nfe 0, Train: 0.6782, Val: 0.5763, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 002, Runtime 0.121939, Loss 1.249313, forward nfe 180, backward nfe 0, Train: 0.6782, Val: 0.5763, Test: 0.7027, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.126798, Loss 0.947823, forward nfe 352, backward nfe 0, Train: 0.8966, Val: 0.7458, Test: 0.7838, Best time: 4.7343\n",
      "Epoch: 004, Runtime 0.144116, Loss 0.692332, forward nfe 536, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 1.7581\n",
      "Epoch: 005, Runtime 0.139766, Loss 0.475014, forward nfe 726, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.155846, Loss 0.357368, forward nfe 922, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.153647, Loss 0.260377, forward nfe 1130, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.162787, Loss 0.186151, forward nfe 1344, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.167717, Loss 0.154467, forward nfe 1552, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.154931, Loss 0.120445, forward nfe 1760, backward nfe 0, Train: 0.9195, Val: 0.7627, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.165887, Loss 0.109748, forward nfe 1974, backward nfe 0, Train: 1.0000, Val: 0.7797, Test: 0.7568, Best time: 0.5508\n",
      "Epoch: 012, Runtime 0.162439, Loss 0.100271, forward nfe 2200, backward nfe 0, Train: 1.0000, Val: 0.7797, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.158796, Loss 0.105190, forward nfe 2432, backward nfe 0, Train: 1.0000, Val: 0.7797, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.156823, Loss 0.094669, forward nfe 2664, backward nfe 0, Train: 1.0000, Val: 0.7797, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.159659, Loss 0.090566, forward nfe 2902, backward nfe 0, Train: 1.0000, Val: 0.7797, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.170168, Loss 0.064058, forward nfe 3140, backward nfe 0, Train: 0.9885, Val: 0.8136, Test: 0.7297, Best time: 1.0773\n",
      "Epoch: 017, Runtime 0.166960, Loss 0.136774, forward nfe 3378, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 6.4725\n",
      "Epoch: 018, Runtime 0.170131, Loss 0.101283, forward nfe 3616, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.176349, Loss 0.154622, forward nfe 3854, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.168201, Loss 0.102789, forward nfe 4092, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.159329, Loss 0.122316, forward nfe 4330, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.157317, Loss 0.099417, forward nfe 4568, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.173648, Loss 0.116603, forward nfe 4800, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.156243, Loss 0.110485, forward nfe 5026, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.157898, Loss 0.054310, forward nfe 5252, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.148707, Loss 0.091236, forward nfe 5472, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.146465, Loss 0.078530, forward nfe 5680, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.152837, Loss 0.094877, forward nfe 5888, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.158863, Loss 0.097373, forward nfe 6096, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.147637, Loss 0.128840, forward nfe 6304, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.145177, Loss 0.097650, forward nfe 6512, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.143457, Loss 0.069150, forward nfe 6708, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.145052, Loss 0.064575, forward nfe 6910, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.145921, Loss 0.085376, forward nfe 7112, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.151854, Loss 0.079902, forward nfe 7320, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.154256, Loss 0.060099, forward nfe 7534, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.153024, Loss 0.044096, forward nfe 7748, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.152856, Loss 0.076363, forward nfe 7962, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.153967, Loss 0.098617, forward nfe 8176, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.145417, Loss 0.093516, forward nfe 8384, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.148343, Loss 0.080954, forward nfe 8598, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.143427, Loss 0.097048, forward nfe 8806, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.150940, Loss 0.067059, forward nfe 9020, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.154819, Loss 0.050950, forward nfe 9228, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.151395, Loss 0.089167, forward nfe 9442, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.153040, Loss 0.075953, forward nfe 9656, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.150816, Loss 0.039165, forward nfe 9870, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.148411, Loss 0.054118, forward nfe 10084, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.148402, Loss 0.048412, forward nfe 10298, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "best val accuracy 0.830508 with test accuracy 0.756757 at epoch 17 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.082344, Loss 1.605544, forward nfe 32, backward nfe 0, Train: 0.6437, Val: 0.6102, Test: 0.6757, Best time: 0.0724\n",
      "Epoch: 002, Runtime 0.088111, Loss 1.259353, forward nfe 138, backward nfe 0, Train: 0.6552, Val: 0.6271, Test: 0.7027, Best time: 0.0656\n",
      "Epoch: 003, Runtime 0.105548, Loss 0.933508, forward nfe 256, backward nfe 0, Train: 0.7931, Val: 0.6949, Test: 0.7568, Best time: 3.8968\n",
      "Epoch: 004, Runtime 0.114602, Loss 0.745176, forward nfe 410, backward nfe 0, Train: 0.8391, Val: 0.7119, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.119555, Loss 0.553075, forward nfe 558, backward nfe 0, Train: 0.8966, Val: 0.7627, Test: 0.8378, Best time: 2.6356\n",
      "Epoch: 006, Runtime 0.115302, Loss 0.426131, forward nfe 712, backward nfe 0, Train: 0.9425, Val: 0.7797, Test: 0.8378, Best time: 0.9287\n",
      "Epoch: 007, Runtime 0.110793, Loss 0.314590, forward nfe 866, backward nfe 0, Train: 0.9540, Val: 0.7966, Test: 0.8378, Best time: 0.9820\n",
      "Epoch: 008, Runtime 0.106025, Loss 0.267794, forward nfe 1020, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 2.9087\n",
      "Epoch: 009, Runtime 0.108324, Loss 0.216004, forward nfe 1162, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 010, Runtime 0.108922, Loss 0.209647, forward nfe 1316, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.111385, Loss 0.179105, forward nfe 1476, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.117312, Loss 0.154677, forward nfe 1636, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 013, Runtime 0.110961, Loss 0.130987, forward nfe 1796, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.119342, Loss 0.091277, forward nfe 1950, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.110398, Loss 0.120109, forward nfe 2110, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.110560, Loss 0.116608, forward nfe 2270, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.114525, Loss 0.113141, forward nfe 2430, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.111033, Loss 0.103325, forward nfe 2584, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.118534, Loss 0.107150, forward nfe 2732, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.114334, Loss 0.116790, forward nfe 2880, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.122201, Loss 0.074296, forward nfe 3028, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.123490, Loss 0.097421, forward nfe 3176, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.107934, Loss 0.193452, forward nfe 3330, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.106748, Loss 0.083958, forward nfe 3484, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.108860, Loss 0.098339, forward nfe 3644, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.111550, Loss 0.088825, forward nfe 3804, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.120888, Loss 0.098477, forward nfe 3964, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.120669, Loss 0.096367, forward nfe 4130, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.114796, Loss 0.082250, forward nfe 4290, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.120922, Loss 0.132203, forward nfe 4456, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.114164, Loss 0.057306, forward nfe 4616, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.111542, Loss 0.079673, forward nfe 4770, backward nfe 0, Train: 0.9770, Val: 0.8475, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.110813, Loss 0.068996, forward nfe 4924, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.108493, Loss 0.087957, forward nfe 5078, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.098373, Loss 0.112632, forward nfe 5226, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.104656, Loss 0.078755, forward nfe 5368, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.113802, Loss 0.050344, forward nfe 5510, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.098221, Loss 0.070306, forward nfe 5646, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.098245, Loss 0.035485, forward nfe 5782, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.103815, Loss 0.042117, forward nfe 5924, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.110355, Loss 0.042875, forward nfe 6060, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.104754, Loss 0.061355, forward nfe 6208, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.103514, Loss 0.054066, forward nfe 6356, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.104019, Loss 0.059482, forward nfe 6504, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.100279, Loss 0.050450, forward nfe 6646, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.106661, Loss 0.061377, forward nfe 6794, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.106432, Loss 0.053184, forward nfe 6942, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.103595, Loss 0.067618, forward nfe 7090, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.105374, Loss 0.040068, forward nfe 7238, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.7838, Best time: 5.0000\n",
      "best val accuracy 0.881356 with test accuracy 0.783784 at epoch 33 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.084718, Loss 1.621705, forward nfe 32, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 1.5929\n",
      "Epoch: 002, Runtime 0.089718, Loss 1.318171, forward nfe 144, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 003, Runtime 0.099634, Loss 0.977943, forward nfe 268, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.097531, Loss 0.707408, forward nfe 398, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.101288, Loss 0.509731, forward nfe 528, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.103836, Loss 0.446839, forward nfe 664, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.097024, Loss 0.355645, forward nfe 794, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.097293, Loss 0.322033, forward nfe 924, backward nfe 0, Train: 0.8276, Val: 0.7458, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.094144, Loss 0.210990, forward nfe 1054, backward nfe 0, Train: 0.9425, Val: 0.7627, Test: 0.7838, Best time: 0.5224\n",
      "Epoch: 010, Runtime 0.094525, Loss 0.179963, forward nfe 1178, backward nfe 0, Train: 0.9540, Val: 0.8136, Test: 0.8108, Best time: 0.5559\n",
      "Epoch: 011, Runtime 0.092154, Loss 0.163735, forward nfe 1296, backward nfe 0, Train: 0.9540, Val: 0.8136, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 012, Runtime 0.094929, Loss 0.117822, forward nfe 1414, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 0.6270\n",
      "Epoch: 013, Runtime 0.094516, Loss 0.129902, forward nfe 1532, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.087128, Loss 0.139895, forward nfe 1650, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.084702, Loss 0.123935, forward nfe 1762, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.088387, Loss 0.113788, forward nfe 1874, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.089317, Loss 0.140769, forward nfe 1986, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.084486, Loss 0.086129, forward nfe 2098, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 019, Runtime 0.083781, Loss 0.100098, forward nfe 2210, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.088263, Loss 0.096961, forward nfe 2322, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.080744, Loss 0.088204, forward nfe 2428, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.086786, Loss 0.088155, forward nfe 2534, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.084315, Loss 0.127122, forward nfe 2640, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.078240, Loss 0.073383, forward nfe 2740, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.077419, Loss 0.107170, forward nfe 2840, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.075468, Loss 0.120098, forward nfe 2940, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.086710, Loss 0.105796, forward nfe 3040, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.076454, Loss 0.106090, forward nfe 3140, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.077599, Loss 0.069630, forward nfe 3240, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.076332, Loss 0.082065, forward nfe 3340, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.086144, Loss 0.049395, forward nfe 3440, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.077209, Loss 0.070772, forward nfe 3540, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.075264, Loss 0.091119, forward nfe 3640, backward nfe 0, Train: 0.9885, Val: 0.8305, Test: 0.7568, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.086640, Loss 0.075941, forward nfe 3740, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.075599, Loss 0.060322, forward nfe 3840, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.076812, Loss 0.065292, forward nfe 3940, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.077346, Loss 0.083551, forward nfe 4040, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.076581, Loss 0.064553, forward nfe 4140, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.084594, Loss 0.051279, forward nfe 4240, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.077520, Loss 0.058547, forward nfe 4340, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.076920, Loss 0.064482, forward nfe 4440, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.087166, Loss 0.060407, forward nfe 4540, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.080099, Loss 0.051088, forward nfe 4640, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.089408, Loss 0.029076, forward nfe 4740, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.076354, Loss 0.053112, forward nfe 4840, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 046, Runtime 0.078542, Loss 0.045920, forward nfe 4940, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.086578, Loss 0.041847, forward nfe 5040, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.078974, Loss 0.041905, forward nfe 5140, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.079010, Loss 0.045183, forward nfe 5240, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8649, Best time: 5.0000\n",
      "best val accuracy 0.864407 with test accuracy 0.864865 at epoch 35 and best time 5.000000\n",
      "True\n",
      "torch.Size([183, 10])\n",
      "using ACMP\n",
      "using ACMP\n",
      "Epoch: 001, Runtime 0.086528, Loss 1.573074, forward nfe 38, backward nfe 0, Train: 0.7931, Val: 0.7288, Test: 0.7568, Best time: 2.9829\n",
      "Epoch: 002, Runtime 0.095424, Loss 1.307379, forward nfe 156, backward nfe 0, Train: 0.8391, Val: 0.7458, Test: 0.8108, Best time: 4.9828\n",
      "Epoch: 003, Runtime 0.112981, Loss 0.957349, forward nfe 298, backward nfe 0, Train: 0.8391, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 004, Runtime 0.120992, Loss 0.683973, forward nfe 452, backward nfe 0, Train: 0.8391, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 005, Runtime 0.120679, Loss 0.540216, forward nfe 612, backward nfe 0, Train: 0.8391, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 006, Runtime 0.125813, Loss 0.432105, forward nfe 778, backward nfe 0, Train: 0.8391, Val: 0.7458, Test: 0.8108, Best time: 5.0000\n",
      "Epoch: 007, Runtime 0.123444, Loss 0.334630, forward nfe 944, backward nfe 0, Train: 0.8851, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 008, Runtime 0.130681, Loss 0.248170, forward nfe 1116, backward nfe 0, Train: 0.8851, Val: 0.7627, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 009, Runtime 0.120194, Loss 0.214537, forward nfe 1288, backward nfe 0, Train: 0.9310, Val: 0.7797, Test: 0.7838, Best time: 0.7890\n",
      "Epoch: 010, Runtime 0.122926, Loss 0.207032, forward nfe 1454, backward nfe 0, Train: 0.9310, Val: 0.7797, Test: 0.7838, Best time: 5.0000\n",
      "Epoch: 011, Runtime 0.114435, Loss 0.158248, forward nfe 1620, backward nfe 0, Train: 0.9770, Val: 0.7966, Test: 0.8378, Best time: 2.0365\n",
      "Epoch: 012, Runtime 0.115969, Loss 0.148343, forward nfe 1774, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 2.1789\n",
      "Epoch: 013, Runtime 0.108504, Loss 0.157938, forward nfe 1916, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 014, Runtime 0.124491, Loss 0.127631, forward nfe 2064, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 015, Runtime 0.110850, Loss 0.163175, forward nfe 2218, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 016, Runtime 0.104463, Loss 0.169371, forward nfe 2360, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 017, Runtime 0.107539, Loss 0.110799, forward nfe 2514, backward nfe 0, Train: 1.0000, Val: 0.8136, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 018, Runtime 0.103606, Loss 0.110457, forward nfe 2650, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 2.6179\n",
      "Epoch: 019, Runtime 0.100156, Loss 0.153116, forward nfe 2780, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 020, Runtime 0.099105, Loss 0.151126, forward nfe 2910, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 021, Runtime 0.106634, Loss 0.106803, forward nfe 3040, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 022, Runtime 0.096734, Loss 0.118790, forward nfe 3170, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 023, Runtime 0.089646, Loss 0.083709, forward nfe 3306, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 024, Runtime 0.088933, Loss 0.091215, forward nfe 3430, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 025, Runtime 0.088492, Loss 0.097955, forward nfe 3554, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 026, Runtime 0.092931, Loss 0.070177, forward nfe 3678, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 027, Runtime 0.092587, Loss 0.079578, forward nfe 3796, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 028, Runtime 0.087420, Loss 0.071862, forward nfe 3914, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 029, Runtime 0.094291, Loss 0.063950, forward nfe 4032, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 030, Runtime 0.093876, Loss 0.114187, forward nfe 4156, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 031, Runtime 0.086118, Loss 0.069595, forward nfe 4280, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 032, Runtime 0.083998, Loss 0.071377, forward nfe 4392, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 033, Runtime 0.085207, Loss 0.118119, forward nfe 4504, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 034, Runtime 0.085355, Loss 0.075087, forward nfe 4616, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 035, Runtime 0.083809, Loss 0.105574, forward nfe 4728, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 036, Runtime 0.084256, Loss 0.077909, forward nfe 4840, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 037, Runtime 0.083770, Loss 0.077930, forward nfe 4952, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 038, Runtime 0.084164, Loss 0.072830, forward nfe 5064, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 039, Runtime 0.094207, Loss 0.061825, forward nfe 5176, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 040, Runtime 0.099250, Loss 0.085914, forward nfe 5306, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 041, Runtime 0.093488, Loss 0.066524, forward nfe 5424, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 042, Runtime 0.089293, Loss 0.037300, forward nfe 5542, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 043, Runtime 0.089140, Loss 0.040450, forward nfe 5654, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 044, Runtime 0.086660, Loss 0.051497, forward nfe 5766, backward nfe 0, Train: 1.0000, Val: 0.8305, Test: 0.8378, Best time: 5.0000\n",
      "Epoch: 045, Runtime 0.084799, Loss 0.089235, forward nfe 5878, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.8649, Best time: 2.4962\n",
      "Epoch: 046, Runtime 0.089330, Loss 0.054258, forward nfe 5990, backward nfe 0, Train: 1.0000, Val: 0.8475, Test: 0.8649, Best time: 5.0000\n",
      "Epoch: 047, Runtime 0.086689, Loss 0.057221, forward nfe 6102, backward nfe 0, Train: 1.0000, Val: 0.8644, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 048, Runtime 0.086264, Loss 0.060780, forward nfe 6214, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "Epoch: 049, Runtime 0.086740, Loss 0.056449, forward nfe 6332, backward nfe 0, Train: 1.0000, Val: 0.8814, Test: 0.8919, Best time: 5.0000\n",
      "best val accuracy 0.881356 with test accuracy 0.891892 at epoch 48 and best time 5.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f21f56c02b0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt['dataset'] = 'texas'\n",
    "\n",
    "opt['function'] = 'ACMP'\n",
    "opt['epoch'] = 50\n",
    "opt['time'] = 5\n",
    "\n",
    "opt['beta'] = 0.5\n",
    "test_acc = []\n",
    "for beta in [0.0,0.1,0.2,0.3,0.4,0.5]:\n",
    "    opt['beta'] = beta\n",
    "    _,_,test_acc_,_,_ = main(opt)\n",
    "    test_acc.append(test_acc_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( [0.0,0.1,0.2,0.3,0.4,0.5],test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model and code is from twitter-research: https://github.com/twitter-research/graph-neural-pde\n",
    "\n",
    "\n",
    "\n",
    "Chamberlain, Ben, et al. \"Grand: Graph neural diffusion.\" International Conference on Machine Learning. PMLR, 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1ff16980caed4c53325d748eca9b3ef767713c55c7da06ab7bd73a6f59e9560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
